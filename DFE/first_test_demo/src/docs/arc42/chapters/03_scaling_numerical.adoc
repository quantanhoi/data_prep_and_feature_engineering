:jbake-title: Scaling Numerical
:jbake-type: page_toc
:jbake-status: published
:jbake-menu: arc42
:jbake-order: 3
:filename: /chapters/03_scaling_numerical.adoc
ifndef::imagesdir[:imagesdir: ../../images]

:toc:



[[section-context-and-scope]]
== Scaling Numerical
This document defines and illustrates concepts around Scaling and Normalizing. Use it as a quick reference when preparing numeric data for machine-learning pipelines.

.Categorical Data and Numerical Data
There are two types of data

Categorical Data:: Data that reflect qualitative characteristics
* Gender
* Hair Color
* Ethnicity
utilising categorical data is essentially a way of assigning number values to inherently qualitative data. For example you can assign Female to 1 and Male to 2
Numerical Data:: Data that are natureally numbers-based, for example age, height and weight....

Within each of these main data types , there are 2 level of measurement

.For categorical data it is

Nominal Data:: Pure identifier, swapping two labels never changes meaning
* *Valid operations -* = ≠, counting frequencies, one‐hot/dummy/feature-hash encodings.
* *Invalid operations –* <, >, +, −, computing mean or standard deviation.
* *Example*: Eye Colour: brown, blue ,green or Country Code DE, FR, US
Ordinal Data:: * Categories imply rank, but the gap between successive ranks is unknown or unequal.
* *Valid operations –* =, ≠, <, >, non-parametric statistics (median, Spearman ρ, Kendall τ).
* *Invalid operations –* Adding / subtracting ranks as if they were numeric distances (unless you trust equal spacing).
* *Example*: high-school < bachelor < master < PhD or strongly disagree (1) … strongly agree (5)

.For numerical data it is 
Interval:: Numeric values are ordered and evenly spaced.
* Zero is arbitrary: it does not mean “none of the quantity.”
* Because zero is arbitrary, ratios are meaningless; you may add or subtract, but not multiply or divide and still retain semantic truth.
* *Example*:  
** Temperature in °C or °F (0 °C is not “no temperature”).
**  Calendar dates measured as “years” (the year 0 is a convention).
** Standardised test scores (e.g., GMAT 200-800).
Ratio data:: Ordered, evenly spaced and the scale has an absolute, natural zero that truly represents “absence.”
* All arithmetic operations—including multiplication, division, percentage change—retain meaning.
* *Example*: 
** Height, weight, length, price, age, count, elapsed time, Kelvin temperature.
** Sensor readings such as luminosity in lux, sound pressure in pascals, CPU cycles.

.Why Scale? Machine-learning optimizer behave best when every input feature has roughly the same dynamic range.
Unscaled data can lead to: 

* divergent or sluggish gradient descent.
* domination of distance metrics by large-magnitude features
* ppor floating-point precision

Most scaling method aim to map feature around *[-1 , 1]*, or to a zero-mean/unit-variance bell shape

=== Linear Scaling Methods

==== Min-Max Scaling
[source,python]
----
x_scaled = (2*x - x_max - x_min)/(x_max - x_min) # maps to [-1, 1]
----

pros:: deterministic bounds, simple inverse transform.
cons:: extremely sensitive to outliers because *x_min/x_max* might be rare anomalies.

.Task Overview
[source, python]
----
'''
Your task: 
    - Open the starter notebook and load housing data
    - Apply a min max scaler to the column total_rooms
    - Verify the transformed column is in range [-1,1]
'''
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# -------------------------------------------------------------
# 1. read the file
# -------------------------------------------------------------
df = pd.read_csv("housing.csv")

# -------------------------------------------------------------
# 2. scale `total_rooms`
# -------------------------------------------------------------
scaler = MinMaxScaler(feature_range=(-1, 1))
df["total_rooms_scaled"] = scaler.fit_transform(df[["total_rooms"]])

#Verify max and min total_room
max_rooms = df["total_rooms"].max()
min_rooms = df["total_rooms"].min()
print("max total rooms: " + str(max_rooms))
print("min total rooms: " + str(min_rooms))

# -------------------------------------------------------------
# 3. write the result
# -------------------------------------------------------------
df.to_csv("housing_scaled.csv", index=False)   # <- new file with the extra column
print("✅  new file written: housing_scaled.csv")
----

With the result output, we can determine the maximum and minimum total_rooms
max total rooms:: 39320.0
min total rooms:: 2.0

Taking a random total_rooms of *880.0*
----
x_scaled = (2*880 - 39320 - 2 ) / (32320 - 2) = -0.95553385
----
Which is a correct scaled number in range [-1, 1]

==== Clipping + Min-Max

* *np.clip(x, lo, hi)* caps insane values.
* Run classic min-max on the clipped data. You keep genuine outliers (they become -1 or +1) while protecting the bulk of the distribution from compression. NOTE: this is *not* the same as *MinMaxScaler(clip=True)* in scikit-learn, which only clips after scaling.