{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e178906f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe : /home/edward99/github/datenvorbearbeitung/.venv/bin/python\n",
      "pandas     : 2.3.1\n",
      "has pyarrow: True\n",
      "has fparq  : True\n"
     ]
    }
   ],
   "source": [
    "import sys, pandas as pd, importlib.util\n",
    "print(\"Python exe :\", sys.executable)\n",
    "print(\"pandas     :\", pd.__version__)\n",
    "print(\"has pyarrow:\", importlib.util.find_spec(\"pyarrow\") is not None)\n",
    "print(\"has fparq  :\", importlib.util.find_spec(\"fastparquet\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "920b6cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    a little extra\n",
      "1           average\n",
      "2              thin\n",
      "3              thin\n",
      "4          athletic\n",
      "Name: body_type, dtype: object\n",
      "(59946, 21)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('okcupid_profiles.parquet', engine='fastparquet')\n",
    "print(df.head()['body_type'])\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66faf0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['body_type_a little extra' 'body_type_athletic' 'body_type_average'\n",
      " 'body_type_curvy' 'body_type_fit' 'body_type_full figured'\n",
      " 'body_type_jacked' 'body_type_overweight' 'body_type_rather not say'\n",
      " 'body_type_skinny' 'body_type_thin' 'body_type_used up' 'body_type_None']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Create OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "# Fit and transform body_type column\n",
    "encoded_data = encoder.fit_transform(df[['body_type']])\n",
    "# Bonus 1: Get feature names\n",
    "feature_names = encoder.get_feature_names_out(['body_type'])\n",
    "print(f\"Feature names: {feature_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d03bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   body_type_a little extra  body_type_athletic  body_type_average  \\\n",
      "0                       1.0                 0.0                0.0   \n",
      "1                       0.0                 0.0                1.0   \n",
      "2                       0.0                 0.0                0.0   \n",
      "3                       0.0                 0.0                0.0   \n",
      "4                       0.0                 1.0                0.0   \n",
      "\n",
      "   body_type_curvy  body_type_fit  body_type_full figured  body_type_jacked  \\\n",
      "0              0.0            0.0                     0.0               0.0   \n",
      "1              0.0            0.0                     0.0               0.0   \n",
      "2              0.0            0.0                     0.0               0.0   \n",
      "3              0.0            0.0                     0.0               0.0   \n",
      "4              0.0            0.0                     0.0               0.0   \n",
      "\n",
      "   body_type_overweight  body_type_rather not say  body_type_skinny  \\\n",
      "0                   0.0                       0.0               0.0   \n",
      "1                   0.0                       0.0               0.0   \n",
      "2                   0.0                       0.0               0.0   \n",
      "3                   0.0                       0.0               0.0   \n",
      "4                   0.0                       0.0               0.0   \n",
      "\n",
      "   body_type_thin  body_type_used up  body_type_None  \n",
      "0             0.0                0.0             0.0  \n",
      "1             0.0                0.0             0.0  \n",
      "2             1.0                0.0             0.0  \n",
      "3             1.0                0.0             0.0  \n",
      "4             0.0                0.0             0.0  \n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with encoded columns\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=feature_names)\n",
    "print(encoded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e01b00c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy coded DataFrame: \n",
      "   body_athletic  body_average  body_curvy  body_fit  body_full figured  \\\n",
      "0          False         False       False     False              False   \n",
      "1          False          True       False     False              False   \n",
      "2          False         False       False     False              False   \n",
      "3          False         False       False     False              False   \n",
      "4           True         False       False     False              False   \n",
      "\n",
      "   body_jacked  body_overweight  body_rather not say  body_skinny  body_thin  \\\n",
      "0        False            False                False        False      False   \n",
      "1        False            False                False        False      False   \n",
      "2        False            False                False        False       True   \n",
      "3        False            False                False        False       True   \n",
      "4        False            False                False        False      False   \n",
      "\n",
      "   body_used up  \n",
      "0         False  \n",
      "1         False  \n",
      "2         False  \n",
      "3         False  \n",
      "4         False  \n"
     ]
    }
   ],
   "source": [
    "#Dummy Coding\n",
    "dummies = pd.get_dummies(df[\"body_type\"], prefix=\"body\", drop_first=True)\n",
    "df_dummy = pd.concat([df.drop(columns=[\"body_type\"]), dummies], axis=1)\n",
    "\n",
    "print(\"Dummy coded DataFrame: \", dummies.head(), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccedf777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect coded DataFrame: \n",
      "  body_athletic body_average body_curvy body_fit body_full figured  \\\n",
      "0            -1           -1         -1       -1                -1   \n",
      "1         False         True      False    False             False   \n",
      "2         False        False      False    False             False   \n",
      "3         False        False      False    False             False   \n",
      "4          True        False      False    False             False   \n",
      "\n",
      "  body_jacked body_overweight body_rather not say body_skinny body_thin  \\\n",
      "0          -1              -1                  -1          -1        -1   \n",
      "1       False           False               False       False     False   \n",
      "2       False           False               False       False      True   \n",
      "3       False           False               False       False      True   \n",
      "4       False           False               False       False     False   \n",
      "\n",
      "  body_used up  \n",
      "0           -1  \n",
      "1        False  \n",
      "2        False  \n",
      "3        False  \n",
      "4        False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n",
      "/tmp/ipykernel_155796/1784277241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dummies.loc[ref_mask, :] = -1\n"
     ]
    }
   ],
   "source": [
    "#Effect Coding\n",
    "import pandas as pd\n",
    "\n",
    "def effect_code(series: pd.Series, prefix=\"x\"):\n",
    "    \"\"\"Return an effect–coded DataFrame (K-1 columns, 1/0/-1).\"\"\"\n",
    "    dummies = pd.get_dummies(series, prefix=prefix, drop_first=True)\n",
    "    # rows that were dropped_first() become the reference --> turn the 0s into -1\n",
    "    ref_mask = (~series.isna()) & (dummies.sum(axis=1) == 0)\n",
    "    dummies.loc[ref_mask, :] = -1\n",
    "    return dummies\n",
    "\n",
    "ec = effect_code(df[\"body_type\"], prefix=\"body\")\n",
    "df_ec = pd.concat([df.drop(columns=\"body_type\"), ec], axis=1)\n",
    "print(\"Effect coded DataFrame: \", ec.head(), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22465d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   body_a little extra  body_athletic  body_average  body_curvy  body_fit  \\\n",
      "0                 True          False         False       False     False   \n",
      "1                False          False          True       False     False   \n",
      "\n",
      "   body_full figured  body_jacked  body_other  body_overweight  body_skinny  \\\n",
      "0              False        False       False            False        False   \n",
      "1              False        False       False            False        False   \n",
      "\n",
      "   body_thin  body_used up  \n",
      "0      False         False  \n",
      "1      False         False  \n"
     ]
    }
   ],
   "source": [
    "# Effect Coding\n",
    "def frequency_cutoff(series: pd.Series, min_count: int = 100) -> pd.Series:\n",
    "    \"\"\"Replace infrequent levels by 'other'.\"\"\"\n",
    "    vc = series.value_counts()\n",
    "    return series.where(series.map(vc) >= min_count, other=\"other\")\n",
    "\n",
    "df[\"body_type_cut\"] = frequency_cutoff(df[\"body_type\"], min_count=200)\n",
    "dummies = pd.get_dummies(df[\"body_type_cut\"], prefix=\"body\")\n",
    "\n",
    "print(dummies.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99ecc55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded DataFrame with OHE: \n",
      "   body_type_athletic  body_type_average  body_type_curvy  body_type_fit  \\\n",
      "0                 0.0                0.0              0.0            0.0   \n",
      "1                 0.0                1.0              0.0            0.0   \n",
      "\n",
      "   body_type_full figured  body_type_jacked  body_type_overweight  \\\n",
      "0                     0.0               0.0                   0.0   \n",
      "1                     0.0               0.0                   0.0   \n",
      "\n",
      "   body_type_skinny  body_type_thin  body_type_used up  body_type_None  \\\n",
      "0               0.0             0.0                0.0             0.0   \n",
      "1               0.0             0.0                0.0             0.0   \n",
      "\n",
      "   body_type_infrequent_sklearn  \n",
      "0                           0.0  \n",
      "1                           0.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(\n",
    "        handle_unknown=\"infrequent_if_exist\",\n",
    "        min_frequency=200,   # absolute cut-off\n",
    "        drop=\"first\"         # optional: keeps dummy coding\n",
    ")\n",
    "X_encoded = ohe.fit_transform(df[[\"body_type\"]])\n",
    "print(\"Encoded DataFrame with OHE: \", pd.DataFrame(X_encoded.toarray(), columns=ohe.get_feature_names_out()).head(2), sep=\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffde8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n",
      "\n",
      "  body_type  liked\n",
      "0   average      1\n",
      "1  athletic      0\n",
      "2   average      1\n",
      "3     curvy      0\n",
      "4  athletic      1\n",
      "5      thin      1\n",
      "6     curvy      0\n",
      "7   average      1\n",
      "liked      non_like  like  total      odds_num\n",
      "body_type                                     \n",
      "athletic          1     1      2  9.999990e-01\n",
      "average           0     3      3  3.000000e+06\n",
      "curvy             2     0      2  0.000000e+00\n",
      "thin              0     1      1  1.000000e+06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# toy sample ─────────────────────────────────────────────\n",
    "df = pd.DataFrame({\n",
    "    \"body_type\": [\"average\", \"athletic\", \"average\", \"curvy\",\n",
    "                  \"athletic\", \"thin\", \"curvy\", \"average\"],\n",
    "    \"liked\":     [1,          0,          1,        0,\n",
    "                  1,          1,       0,        1]       # 1 = liked, 0 = skipped\n",
    "})\n",
    "print(\"Sample DataFrame:\\n\", df, sep=\"\\n\")\n",
    "\n",
    "# 1) build the 2-way contingency table\n",
    "ct = (\n",
    "    pd.crosstab(df.body_type, df.liked)\n",
    "      .rename(columns={0: \"non_like\", 1: \"like\"})\n",
    "      .assign(                                  # extra stats we care about\n",
    "          total     = lambda x: x.like + x.non_like,\n",
    "          # numerator of odds ratio  = like / (non_like + ε)\n",
    "          odds_num  = lambda x: x.like / (x.non_like + 1e-6)\n",
    "      )\n",
    ")\n",
    "\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a675c424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-fold CV accuracy: 0.75\n",
      "\n",
      "encoded test rows:\n",
      "[[0.e+00 0.e+00]\n",
      " [2.e+00 2.e+06]]\n",
      "\n",
      "predicted probabilities (like=1):\n",
      "[0.25932963 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward99/github/datenvorbearbeitung/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bin-count (target) encoding demo\n",
    "✓ high-cardinality categorical → dense numeric columns\n",
    "✓ no target leakage inside CV\n",
    "✓ works end-to-end in an sklearn Pipeline\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. toy data ─ exactly the little body_type / liked table you typed\n",
    "# ---------------------------------------------------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"body_type\": [\"average\", \"athletic\", \"average\", \"curvy\",\n",
    "                  \"athletic\", \"thin\", \"curvy\", \"average\"],\n",
    "    \"liked\":     [1, 0, 1, 0, 1, 1, 0, 1]\n",
    "})\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. helpers to create look-up tables   (fit on *training* data only!)\n",
    "# ---------------------------------------------------------------------\n",
    "def _build_lookups(series: pd.Series, y: pd.Series,\n",
    "                   min_count: int = 1, eps: float = 1e-6):\n",
    "    \"\"\"return two dicts: positive counts, odds numerator\"\"\"\n",
    "    vc = series.value_counts()\n",
    "    safe = series.where(series.map(vc) >= min_count, other=\"other\")\n",
    "\n",
    "    ct = pd.crosstab(safe, y)              # rows = category, cols = {0,1}\n",
    "    ct = ct.rename(columns={0: \"neg\", 1: \"pos\"})\n",
    "\n",
    "    pos_cnt  = ct[\"pos\"].to_dict()\n",
    "    neg_cnt  = ct[\"neg\"].to_dict()\n",
    "    odds_num = {k: pos_cnt[k] / (neg_cnt.get(k, 0) + eps) for k in ct.index}\n",
    "    return pos_cnt, odds_num\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. proper sklearn transformer   (stateless once look-ups are given)\n",
    "# ---------------------------------------------------------------------\n",
    "class BinCountEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Replace a single categorical column by\n",
    "    – positive count\n",
    "    – odds-numerator  (= pos / neg)\n",
    "    \"\"\"\n",
    "    def __init__(self, min_count: int = 30):\n",
    "        self.min_count = min_count\n",
    "        # placeholders – filled in fit()\n",
    "        self._pos_lookup = None\n",
    "        self._odds_lookup = None\n",
    "        self._feat_names = np.array([\"bin_pos_cnt\", \"bin_odds_num\"])\n",
    "\n",
    "    # sklearn API ------------------------------------------------------\n",
    "    def fit(self, X, y):\n",
    "        # X arrives as DataFrame with ONE column\n",
    "        cat = X.iloc[:, 0]\n",
    "        self._pos_lookup, self._odds_lookup = _build_lookups(\n",
    "            cat, y, min_count=self.min_count\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        cat = X.iloc[:, 0]\n",
    "        pos  = cat.map(self._pos_lookup).fillna(self._pos_lookup.get(\"other\", 0))\n",
    "        odds = cat.map(self._odds_lookup).fillna(self._odds_lookup.get(\"other\", 0.0))\n",
    "        return np.vstack([pos, odds]).T             # shape (n_samples, 2)\n",
    "\n",
    "    def get_feature_names_out(self, in_names=None):\n",
    "        return self._feat_names\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. pipeline  → safe CV without leakage\n",
    "# ---------------------------------------------------------------------\n",
    "pipe = Pipeline([\n",
    "    (\"bin\",  BinCountEncoder(min_count=1)),   # 1 for this tiny example\n",
    "    (\"clf\",  LogisticRegression(solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "print(\"4-fold CV accuracy:\",\n",
    "      cross_val_score(pipe, df[[\"body_type\"]], df[\"liked\"],\n",
    "                      cv=4, scoring=\"accuracy\").mean())\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. train / inference example\n",
    "# ---------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[[\"body_type\"]], df[\"liked\"], test_size=0.25, random_state=0\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"\\nencoded test rows:\")\n",
    "print(pipe.named_steps[\"bin\"].transform(X_test))\n",
    "\n",
    "print(\"\\npredicted probabilities (like=1):\")\n",
    "print(pipe.predict_proba(X_test)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab143f70",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "feature names must be strings",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# choose the vector width (power of two is common). 8 buckets   for the demo\u001b[39;00m\n\u001b[32m      6\u001b[39m hasher = FeatureHasher(n_features=\u001b[32m8\u001b[39m, input_type=\u001b[33m\"\u001b[39m\u001b[33mdict\u001b[39m\u001b[33m\"\u001b[39m, alternate_sign=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m hashed = \u001b[43mhasher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_hash\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# sparse CSR matrix\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ── 3. wrap for readability  ──────────────────────────────────────────\u001b[39;00m\n\u001b[32m     10\u001b[39m hashed_df = pd.DataFrame(\n\u001b[32m     11\u001b[39m     hashed.toarray().astype(\u001b[38;5;28mint\u001b[39m),          \u001b[38;5;66;03m# dense for the print-out\u001b[39;00m\n\u001b[32m     12\u001b[39m     columns=[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hashed.shape[\u001b[32m1\u001b[39m])]\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/datenvorbearbeitung/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/datenvorbearbeitung/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/_hash.py:184\u001b[39m, in \u001b[36mFeatureHasher.transform\u001b[39m\u001b[34m(self, raw_X)\u001b[39m\n\u001b[32m    181\u001b[39m     raw_X_ = chain([first_raw_X], raw_X)\n\u001b[32m    182\u001b[39m     raw_X = (((f, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m raw_X_)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m indices, indptr, values = \u001b[43m_hashing_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malternate_sign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    186\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m n_samples = indptr.shape[\u001b[32m0\u001b[39m] - \u001b[32m1\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_samples == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/datenvorbearbeitung/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/_hashing_fast.pyx:56\u001b[39m, in \u001b[36msklearn.feature_extraction._hashing_fast.transform\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: feature names must be strings"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "# ── 2. FeatureHasher expects a list / iterable of {feature_name: value} dicts\n",
    "to_hash = df['body_type'].apply(lambda s: {s: 1})\n",
    "\n",
    "# choose the vector width (power of two is common). 8 buckets   for the demo\n",
    "hasher = FeatureHasher(n_features=8, input_type=\"dict\", alternate_sign=True)\n",
    "hashed = hasher.transform(to_hash)        # sparse CSR matrix\n",
    "\n",
    "# ── 3. wrap for readability  ──────────────────────────────────────────\n",
    "hashed_df = pd.DataFrame(\n",
    "    hashed.toarray().astype(int),          # dense for the print-out\n",
    "    columns=[f\"h{i}\" for i in range(hashed.shape[1])]\n",
    ")\n",
    "print(\"hashed representation:\\n\", hashed_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
