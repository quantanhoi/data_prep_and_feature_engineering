:jbake-title: Embedding
:jbake-type: page_toc
:jbake-status: published
:jbake-menu: arc42
:jbake-order: 5
:filename: /chapters/05_embedding.adoc
ifndef::imagesdir[:imagesdir: ../../images]

:toc:

[[section-embedding]]
== Embedding

=== What is Embedding?

Embedding is a technique that maps discrete categorical data (like words, categories, or IDs) into dense, continuous vector representations. Instead of using sparse one-hot encoding, embeddings represent each category as a learnable dense vector of real numbers.

==== Key Concepts

* **Dense Representation**: Each category becomes a vector of floating-point numbers
* **Learnable Parameters**: Embedding vectors are learned during training through backpropagation
* **Dimensionality Reduction**: Maps high-dimensional sparse data to lower-dimensional dense space
* **Semantic Relationships**: Similar items can have similar embedding vectors

=== Why Use Embeddings?

==== Problems with One-Hot Encoding

* **High Dimensionality**: For 10,000 words, you need 10,000-dimensional vectors
* **Sparsity**: Vectors are mostly zeros (only one 1 per vector)
* **No Relationships**: Cannot capture semantic similarities between categories
* **Memory Inefficient**: Wastes computational resources

==== Benefits of Embeddings

* **Compact Representation**: 10,000 words → 100-dimensional embeddings
* **Dense Vectors**: All values are meaningful floating-point numbers
* **Semantic Meaning**: Similar words/categories get similar vectors
* **Efficient Training**: Faster computation and less memory usage

=== Categorical Embedding Example

==== Dataset: Baby Weight Plurality

The notebook demonstrates embedding with baby birth plurality categories:

[source,python]
----
CLASSES = {
    'Single(1)': 0,
    'Multiple(2+)': 1,
    'Twins(2)': 2,
    'Triplets(3)': 3,
    'Quadruplets(4)': 4,
    'Quintuplets(5)': 5
}
----

==== Creating the Embedding Layer

[source,python]
----
EMBED_DIM = 2  # Each category → 2-dimensional vector
embedding_layer = layers.Embedding(
    input_dim=N_CLASSES,    # 6 categories (0-5)
    output_dim=EMBED_DIM    # 2-dimensional output vectors
)
----

==== Lookup Table Concept

The embedding layer creates a learnable lookup table:

----
Index 0 (Single)      → [0.1, -0.3]
Index 1 (Multiple)    → [-0.2, 0.7]
Index 2 (Twins)       → [0.4, 0.1]
Index 3 (Triplets)    → [-0.1, -0.5]
Index 4 (Quadruplets) → [0.6, 0.2]
Index 5 (Quintuplets) → [0.0, -0.4]
----

==== Output Shape

* **Input**: `[0, 2, 1, 0, 3]` (integer category IDs)
* **Output**: `(batch_size, embed_dim)` → `(5, 2)` tensor
* **Values**: Dense floating-point vectors

=== Text Embedding Example

==== Task: Website Classification

Predict which website (GitHub, NY Times, TechCrunch) an article comes from based on its title.

==== Text Preprocessing Pipeline

[source,python]
----
# 1. Build vocabulary from all titles
tokenizer = Tokenizer()
tokenizer.fit_on_texts(titles_df.title)

# 2. Convert text to integer sequences
integerized_titles = tokenizer.texts_to_sequences(titles_df.title)

# 3. Pad sequences to fixed length
padded_sequences = pad_sequences(sequences, MAX_LEN, padding='post')
----

==== Vocabulary Statistics

* **VOCAB_SIZE**: Total number of unique words
* **DATASET_SIZE**: Number of titles in dataset
* **MAX_LEN**: Length of longest title (for padding)

==== Word-to-Integer Mapping

[source,python]
----
# Decode integer back to word
token_id = integerized_titles[0][0]
word = tokenizer.index_word[token_id]
print(f"{token_id} → {word}")

# Decode entire sequence
decoded = [tokenizer.index_word[id] for id in integerized_titles[0]]
----

==== Neural Network Architecture

[source,python]
----
def build_dnn_model(embed_dim):
    model = models.Sequential([
        # Embedding Layer: word_id → dense_vector
        layers.Embedding(VOCAB_SIZE + 1, embed_dim, input_shape=[MAX_LEN]),
        
        # Average Pooling: sequence → single vector
        layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)),
        
        # Classification: vector → probabilities
        layers.Dense(N_CLASSES, activation='softmax')
    ])
    return model
----

==== Layer-by-Layer Transformation

1. **Embedding Layer**
   * Input: `(batch_size, MAX_LEN)` - sequences of word indices
   * Output: `(batch_size, MAX_LEN, embed_dim)` - sequences of dense vectors

2. **Lambda Layer (Average Pooling)**
   * Input: `(batch_size, MAX_LEN, embed_dim)`
   * Output: `(batch_size, embed_dim)` - single vector per sample
   * Function: Averages all word embeddings in the sequence

3. **Dense Layer**
   * Input: `(batch_size, embed_dim)`
   * Output: `(batch_size, N_CLASSES)` - probability distribution
   * Activation: Softmax for multi-class classification

=== Training Process

==== Data Preparation

[source,python]
----
# Split data (80% train, 20% validation)
N_TRAIN = int(DATASET_SIZE * 0.8)
titles_train, sources_train = titles_df.title[:N_TRAIN], titles_df.source[:N_TRAIN]
titles_valid, sources_valid = titles_df.title[N_TRAIN:], titles_df.source[N_TRAIN:]

# Convert to model inputs
X_train = create_sequences(titles_train)  # Padded integer sequences
Y_train = encode_labels(sources_train)    # One-hot encoded labels
----

==== Model Training

[source,python]
----
dnn_model = build_dnn_model(embed_dim=10)
dnn_history = dnn_model.fit(
    X_train, Y_train,
    epochs=100,
    batch_size=300,
    validation_data=(X_valid, Y_valid),
    callbacks=[callbacks.EarlyStopping(patience=0)]
)
----

==== Training Output

* **Loss Curves**: Training and validation loss over epochs
* **Accuracy Curves**: Training and validation accuracy over epochs
* **Model Summary**: Layer shapes and parameter counts

=== Key Insights

==== Embedding Learning

1. **Random Initialization**: Embeddings start with random values
2. **Backpropagation**: Vectors are updated during training
3. **Task-Specific**: Embeddings learn representations useful for the specific task
4. **Semantic Capture**: Similar words/categories develop similar embeddings

==== Practical Considerations

* **Embedding Dimension**: Balance between expressiveness and efficiency
* **Vocabulary Size**: `VOCAB_SIZE + 1` to account for unknown words (index 0)
* **Padding**: Zero-padding ensures fixed-length inputs for neural networks
* **Sequence Length**: Longer sequences capture more context but increase computation

==== Performance Benefits

* **Faster Training**: Dense operations are more efficient than sparse
* **Better Generalization**: Learned representations often transfer to similar tasks
* **Memory Efficiency**: Smaller model size compared to one-hot encoding
* **Semantic Understanding**: Model learns meaningful word relationships

=== Common Use Cases

* **Natural Language Processing**: Word embeddings for text classification, sentiment analysis
* **Recommendation Systems**: User/item embeddings for collaborative filtering  
* **Computer Vision**: Feature embeddings for image classification
* **Categorical Features**: Product categories, user demographics, geographic regions
* **Time Series**: Temporal embeddings for sequential data

=== Advanced Techniques

==== Pre-trained Embeddings

[source,python]
----
# Using pre-trained word vectors (Word2Vec, GloVe)
embedding_layer = layers.Embedding(
    vocab_size, 
    embed_dim,
    weights=[pretrained_weights],
    trainable=False  # Freeze pre-trained weights
)
----

==== Embedding Regularization

* **Dropout**: Apply dropout to embeddings to prevent overfitting
* **Weight Decay**: L2 regularization on embedding parameters
* **Embedding Size**: Start small (50-100 dims) and increase if needed

=== Troubleshooting

==== Common Issues

* **Cold Start**: New categories not seen during training
* **Dimensionality**: Too high → overfitting, too low → underfitting
* **Vocabulary Size**: Out-of-vocabulary words need special handling
* **Initialization**: Random vs. pre-trained initialization trade-offs

==== Best Practices

* Use `mask_zero=True` for variable-length sequences
* Reserve index 0 for padding tokens
* Monitor embedding norms during training
* Visualize embeddings using t-SNE or PCA for debugging

=== Multimodal Extensions

Embeddings can be combined with other data types:

[source,python]
----
# Text + Categorical + Numerical features
text_branch = Embedding(vocab_size, 64)(text_input)
categorical_branch = Embedding(n_categories, 32)(categorical_input)
numerical_branch = Dense(16)(numerical_input)

# Concatenate all features
combined = concatenate([text_branch, categorical_branch, numerical_branch])
----


