:jbake-title: Categorical Variable
:jbake-type: page_toc
:jbake-status: published
:jbake-menu: arc42
:jbake-order: 4
:filename: /chapters/04_categorical.adoc
ifndef::imagesdir[:imagesdir: ../../images]

:toc:



[[section-solution-strategy]]
== Categorical Variable
Categorical variables describe qualitative phenomena - they represent characteristics, qualities, or group memberships rather than measurable quantities. This is the fundamental distinction from quantitative variables.

Quantitative Variables::
Quantitative Variables measure amounts
- Stock price: $145.32
- Temperature: 25°C
- Height: 175 cm
- Age: 32 years

Categorical Variables::
- Industry sector: "Technology", "Healthcare", "Finance"
- Customer satisfaction: "Poor", "Good", "Excellent"
- Country of origin: "USA", "Germany", "Japan"


.Type of Categorical Variables
1. Ordered (Ordinal) Categories - have a meaningful sequence:
- Education level: "High School" < "Bachelor's" < "Master's" < "PhD"
- Customer rating: "Poor" < "Fair" < "Good" < "Excellent"
- T-shirt sizes: "Small" < "Medium" < "Large" < "X-Large"

2. Unordered (Nominal) Categories - no inherent order:
- Cuisine type: "French", "Indian", "American", "Italian"
- Blood type: "A", "B", "AB", "O"
- Car manufacturer: "Toyota", "Ford", "BMW"


.Numbers can be categorical
Categorical variables are independent from data type - even numerical values can represent categories when they don't have mathematical meaning.

Example:
**Postal Codes:** "64295" (Darmstadt) and "55118" (Mainz)
(Darmstadt is not 9177 better than Mainz)
These numbers are just labels; mathematical operations don't make sense

**Other Numerical Categories:**

- Employee ID: 10234, 10235, 10236 (not meaningful to add or average)
- Product codes: 1001, 2005, 3010
- Phone area codes: 212, 415, 617
- Jersey numbers in sports: 23, 45, 99


.Machine Learning models for categorical data
- Decision trees can use categorical values as cut-offs
- CatBoost got its name from efficiently dealing with categorical variables
- Naive Bayes models work with contingency tables based on categorical values
**However** most models expect numerical data

=== One Hot Encoding

Why One Hot Encoding improves machine learning performance:
One-hot encoding is a crucial step in data preparation for machine learning algorithms. It involves converting categorical data into a numerical format that algorithms can understand. This transformation is essential because most machine learning models require numerical input to perform calculations and make predictions.

.One-hot encoding for categorical variables
Categorical data, by its nature, cannot be directly used by machine learning algorithms. These algorithms are designed to work with numerical data, and categorical variables need to be transformed into a format that these algorithms can process. Assigning arbitrary numerical values to to categorical data can lead to incorrect interpretations by the algorithm, as these values may imply a false order or hierarchy among the categories.  For instance, if we assign the values 0, 1, and 2 to the categories "UK", "French", and "US", respectively, the algorithm may incorrectly assume that "US" is twice as significant as "UK" or that "French" is midway between "UK" and "US".

.How it works
For each unique category in the original variable:
1. Create a new binary column
2. Set value to 1 where that category is present
3. Set value to 0 where that category is absent

[source,python]
----
# Example transformation
Original:          One-Hot Encoded:
City               DA  FFM  MZ
----               -------------
DA        →        1   0    0
FFM       →        0   1    0
MZ        →        0   0    1
DA        →        1   0    0
----

Each category gets its own column, and the presence of that category is indicated by a 1, while absence is indicated by a 0. This allows machine learning algorithms to treat each category independently without implying any order or hierarchy.

.Implementation in python using sklearn
[source,python]
----
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# Create sample data
data = pd.DataFrame({
    'city': ['New York', 'London', 'Paris', 'New York', 'London']
})

# Initialize encoder
encoder = OneHotEncoder(sparse_output=False)

# Fit and transform
encoded = encoder.fit_transform(data[['city']])

# Get feature names
feature_names = encoder.get_feature_names_out(['city'])
encoded_df = pd.DataFrame(encoded, columns=feature_names)
----

.Using pandas get_dummies
[source,python]
----
# Simple one-hot encoding with pandas
encoded = pd.get_dummies(data, columns=['city'])

# With prefix
encoded = pd.get_dummies(data, columns=['city'], prefix='city')
----

.Example with cupid parquet file
[source,python]
----
import pandas as pd
df = pd.read_parquet('okcupid_profiles.parquet', engine='fastparquet')
print(df.head()['body_type'])
"""
0    a little extra
1           average
2              thin
3              thin
4          athletic
Name: body_type, dtype: object
"""

from sklearn.preprocessing import OneHotEncoder
# Create OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
# Fit and transform body_type column
encoded_data = encoder.fit_transform(df[['body_type']])
# Bonus 1: Get feature names
feature_names = encoder.get_feature_names_out(['body_type'])
print(f"Feature names: {feature_names}")
"""
Feature names: ['body_type_a little extra' 'body_type_athletic' 'body_type_average'
 'body_type_curvy' 'body_type_fit' 'body_type_full figured'
 'body_type_jacked' 'body_type_overweight' 'body_type_rather not say'
 'body_type_skinny' 'body_type_thin' 'body_type_used up' 'body_type_None']
"""
# Create DataFrame with encoded columns
encoded_df = pd.DataFrame(encoded_data, columns=feature_names)
print(encoded_df.head())
----

.Output:
----
body_type_a little extra  body_type_athletic  body_type_average  \
0                       1.0                 0.0                0.0   
1                       0.0                 0.0                1.0   
2                       0.0                 0.0                0.0   
3                       0.0                 0.0                0.0   
4                       0.0                 1.0                0.0   

   body_type_curvy  body_type_fit  body_type_full figured  body_type_jacked  \
0              0.0            0.0                     0.0               0.0   
1              0.0            0.0                     0.0               0.0   
2              0.0            0.0                     0.0               0.0   
3              0.0            0.0                     0.0               0.0   
4              0.0            0.0                     0.0               0.0   

   body_type_overweight  body_type_rather not say  body_type_skinny  \
0                   0.0                       0.0               0.0   
1                   0.0                       0.0               0.0   
2                   0.0                       0.0               0.0   
3                   0.0                       0.0               0.0   
4                   0.0                       0.0               0.0   

   body_type_thin  body_type_used up  body_type_None  
0             0.0                0.0             0.0  
1             0.0                0.0             0.0  
2             1.0                0.0             0.0  
3             1.0                0.0             0.0  
4             0.0                0.0             0.0
----


.Advantages of One-Hot Encoding

1. Prevents models from assuming false relationships between categories
* Example: Without encoding, a model might think "City_ID 3" > "City_ID 1"

2. **Model Compatibility:** 
* Most ML algorithms require numerical input
* Enables use of linear models, neural networks, SVMs, etc.

3. **Clear Representation**
* Each category is independently represented
* Easy to interpret which category is active

4. **Handles Non-Ordinal Categories**
* Perfect for nominal variables with no natural order
* Examples: Colors, cities, product types



.Disadvantages of One-Hot Encoding

1. **Curse of Dimensionality**: WARNING: High cardinality features can explode feature space.
1000 unique cities → 1,000 new columns
10000 product IDs → 10,000 new columns


2. **Sparse Data**
* Most values are 0 (only one 1 per row)
* Increased memory usage
* Computational inefficiency


3. **Multicollinearity**
* Columns are linearly dependent: sum of all columns = 1
* Can cause numerical instability in some algorithms
* Solution: Drop one column (dummy coding)


4. **Incomplete Vocabulary Problem** 
[source,python]
----
# Training data sees: ['NYC', 'LA', 'Chicago']
# Test data has: ['NYC', 'LA', 'Boston']  # Boston causes error!

# Solution: Use handle_unknown='ignore'
encoder = OneHotEncoder(handle_unknown='ignore')
----

.When to Use One-Hot Encoding
1. **Low to Medium Cardinality** (<100 unique values) 
2. **Nominal Categories** (no natural order)
   * Colors: Red, Blue, Green
   * Countries: USA, UK, Japan
   * Product categories: Electronics, Clothing, Food

3. **Stable Categories** (unlikely to see new values)
4. **Model Requires Numerical Input**
   * Linear/Logistic Regression
   * Neural Networks
   * Support Vector Machines

5. **Interpretability Matters**
   * Each coefficient represents one category's effect

**Example Use Cases:**
[source,python]
----
# Good candidates for one-hot encoding
df['day_of_week']     # 7 values: Mon-Sun
df['region']          # 5 values: North, South, East, West, Central  
df['product_type']    # 10 values: known product categories
df['education_level'] # 6 values: HS, Bachelor's, Master's, etc.
----


.❌ Avoid When:

1. **High Cardinality** (>1000 unique values) [[6]]
   * User IDs, Device IDs
   * ZIP codes, IP addresses
   * URLs, email domains

2. **Ordinal Categories** (natural order exists)
   * Ratings: Poor, Fair, Good, Excellent
   * Sizes: S, M, L, XL
   * Education levels (if order matters)

3. **Text Data** (use embeddings instead)
4. **Continuous Variables** (use scaling/binning)
5. **Memory Constraints** (sparse matrices become too large)

=== Dummy Coding
This is the same as one-hot encoding, but with one less column. Because that “extra” indicator column never adds information—it is a perfect linear copy of the others.
Example with three categories
body_type ∈ {average, athletic, thin}
One-hot (drop_first=False) gives you three columns:
[source,python]
----
          average   athletic   thin  
row 1        1         0         0  
row 2        0         1         0  
row 3        0         0         1  
----
Dummy coding (drop_first=True) gives you two columns:
[source,python]
----
          athletic   thin      ← K-1 columns
row 1        0         0        ← both zero  ⇒ reference = “average”
row 2        1         0
row 3        0         1
----

* You have the same K possible patterns, one of them is now all-zeros vector
* The Matrix is full rank, so coefficients estimation works
* Each coefficient is the effect relative to the reference (“average”).

.Example coding
[source,python]
----
#Dummy Coding
dummies = pd.get_dummies(df["body_type"], prefix="body", drop_first=True)
# Concat to original DataFrame and drop original column
df_dummy = pd.concat([df.drop(columns=["body_type"]), dummies], axis=1)

print("Dummy coded DataFrame: ", dummies.head(), sep="\n")
----

.Output:
[source, python]
----
Dummy coded DataFrame: 
   body_athletic  body_average  body_curvy  body_fit  body_full figured  \
0          False         False       False     False              False   
1          False          True       False     False              False   
2          False         False       False     False              False   
3          False         False       False     False              False   
4           True         False       False     False              False   

   body_jacked  body_overweight  body_rather not say  body_skinny  body_thin  \
0        False            False                False        False      False   
1        False            False                False        False      False   
2        False            False                False        False       True   
3        False            False                False        False       True   
4        False            False                False        False      False   

   body_used up  
0         False  
1         False  
2         False  
3         False  
4         False  
----


=== Effect Coding
Effect coding starts from ordinary one-hot/dummy columns and then re-labels the reference group.
.Dummy Coding

* K-1 Columns
* reference rows = all 0

.Effect Coding

* Same K-1 Columns
* reference rows = all -1
* every non-reference row keeps a single +1 and the rest 0
[source,python]
----
#Effect Coding
import pandas as pd

def effect_code(series: pd.Series, prefix="x"):
    """Return an effect–coded DataFrame (K-1 columns, 1/0/-1)."""
    dummies = pd.get_dummies(series, prefix=prefix, drop_first=True)
    # rows that were dropped_first() become the reference --> turn the 0s into -1
    ref_mask = (~series.isna()) & (dummies.sum(axis=1) == 0)
    dummies.loc[ref_mask, :] = -1
    return dummies

ec = effect_code(df["body_type"], prefix="body")
df_ec = pd.concat([df.drop(columns="body_type"), ec], axis=1)
print("Effect coded DataFrame: ", ec.head(), sep="\n")
----

.Output:
[source,python]
----
#Effect Coding
import pandas as pd

def effect_code(series: pd.Series, prefix="x"):
    """Return an effect–coded DataFrame (K-1 columns, 1/0/-1)."""
    dummies = pd.get_dummies(series, prefix=prefix, drop_first=True)
    # rows that were dropped_first() become the reference --> turn the 0s into -1
    ref_mask = (~series.isna()) & (dummies.sum(axis=1) == 0)
    dummies.loc[ref_mask, :] = -1
    return dummies

ec = effect_code(df["body_type"], prefix="body")
df_ec = pd.concat([df.drop(columns="body_type"), ec], axis=1)
print("Effect coded DataFrame: ", ec.head(), sep="\n")
----

=== Frequency Cut-Off
Take a high-cardinality (many different values) categorical column, pick a threshold (absolute count or relative share).
.Take an example

* body_type column has 300 unique values
* the most common 50 types take up 90% of the data
* we can replace the remaining 250 types with a single "other" category
* This is where we can use frequency cut-off
* min_count = 200 means that if the frequency of a category is below 200, it will be replaced with "other"

.Example with function
[source,python]
----
# Effect Coding
def frequency_cutoff(series: pd.Series, min_count: int = 100) -> pd.Series:
    """Replace infrequent levels by 'other'."""
    vc = series.value_counts()
    return series.where(series.map(vc) >= min_count, other="other")

df["body_type_cut"] = frequency_cutoff(df["body_type"], min_count=200)
dummies = pd.get_dummies(df["body_type_cut"], prefix="body")

print(dummies.head(2))
----
.Output:
[source,python]
----
body_a little extra  body_athletic  body_average  body_curvy  body_fit  \
0                 True          False         False       False     False   
1                False          False          True       False     False   

   body_full figured  body_jacked  body_other  body_overweight  body_skinny  \
0              False        False       False            False        False   
1              False        False       False            False        False   

   body_thin  body_used up  
0      False         False  
1      False         False
----

.Example with sklearn
[source,python]
----
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(
        handle_unknown="infrequent_if_exist",
        min_frequency=200,   # absolute cut-off
        drop="first"         # optional: keeps dummy coding
)
X_encoded = ohe.fit_transform(df[["body_type"]])
print("Encoded DataFrame with OHE: ", pd.DataFrame(X_encoded.toarray(), columns=ohe.get_feature_names_out()).head(), sep="\n")
----

.When NOT to use it
If the tail categories really are business-critical (e.g. fraud labels, rare diseases), you must keep them or model them differently.

.Output:
[source,python]
----
Encoded DataFrame with OHE: 
   body_type_athletic  body_type_average  body_type_curvy  body_type_fit  \
0                 0.0                0.0              0.0            0.0   
1                 0.0                1.0              0.0            0.0   

   body_type_full figured  body_type_jacked  body_type_overweight  \
0                     0.0               0.0                   0.0   
1                     0.0               0.0                   0.0   

   body_type_skinny  body_type_thin  body_type_used up  body_type_None  \
0               0.0             0.0                0.0             0.0   
1               0.0             0.0                0.0             0.0   

   body_type_infrequent_sklearn  
0                           0.0  
1                           0.0
----



=== Feature Hashing

Feature hashing, also known as the "hashing trick", is a technique for converting categorical variables (especially those with high cardinality) into a fixed-length numerical vector using a hash function. It is widely used in large-scale machine learning and natural language processing tasks.

.What is Feature Hashing?
Feature hashing maps each category (or token) to one of a fixed number of columns (buckets) using a hash function. Instead of creating a column for every unique category (as in one-hot encoding), all categories are distributed across a predefined number of columns. This approach is highly memory-efficient and scalable.

.How it works
1. Choose the number of output features (buckets), e.g., 1024.
2. For each category value, compute its hash and map it to a bucket index.
3. If multiple categories hash to the same bucket (collision), their values are combined (typically summed).
4. The resulting feature vector has a fixed length, regardless of the number of unique categories.

.Example
[source,python]
----
from sklearn.feature_extraction import FeatureHasher
# Example: Hashing city names into 4 buckets
data = [{'city': 'New York'}, {'city': 'London'}, {'city': 'Paris'}, {'city': 'New York'}]
hasher = FeatureHasher(n_features=4, input_type='dict')
hashed_features = hasher.transform(data)
print(hashed_features.toarray())
----

.Another Example with Cupid location
[source,python]
----
import pandas as pd
from sklearn.feature_extraction import FeatureHasher
df = pd.read_parquet('okcupid_profiles.parquet', engine='fastparquet')
print(df['location'].head())


# 2. Coerce location to str, fill missing with a sentinel
location_str = df["location"].fillna("missing").astype(str)

# 3. Build list-of-strings format  (simplest)
samples = location_str.to_list()          # length = n_rows
samples = [[s] for s in samples]      # each sample must be an *iterable* of str

# 4. Feature hashing: 2⁸ = 256 buckets with signed hash
hasher = FeatureHasher(
    n_features=256,          # power of two makes modulo cheap; tune as needed
    input_type="string",
    alternate_sign=True      # +1 / –1 collisions cancel instead of always adding
)

hashed = hasher.transform(samples)     # sparse CSR matrix  (n_rows × 256)

# 5. Wrap in a DataFrame (dense just for the demo print-out)
hashed_df = pd.DataFrame(
    hashed.toarray(),                  # keep sparse in real pipelines!
    columns=[f"hash_{i}" for i in range(hashed.shape[1])],
    index=df.index
)
print("first 3 hashed rows:")
print(hashed_df.head(3))

# 6. Stick back onto the original frame (drop location if you’re done)
df_hashed = pd.concat([df.drop(columns="location"), hashed_df], axis=1)
print("final shape with hashed columns ->", df_hashed.shape)
----

.Output:
[source,python]
----
first 3 hashed rows:
   hash_0  hash_1  hash_2  hash_3  hash_4  hash_5  hash_6  hash_7  hash_8  \
0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   
1     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   
2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   

   hash_9  ...  hash_246  hash_247  hash_248  hash_249  hash_250  hash_251  \
0     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   
1     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   
2     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   

   hash_252  hash_253  hash_254  hash_255  
0       0.0       0.0       0.0       0.0  
1       0.0       0.0       0.0       0.0  
2       0.0       0.0       0.0       0.0  

[3 rows x 256 columns]
final shape with hashed columns -> (59946, 276)
----

.Explain:

* Instead of 100000 like one-hot encoding, where the first category is true and the rest are false, with feature hashing, this can be 100101.
* In the above example, the location column was hashed into 256 buckets. which is defined by the `n_features` parameter.

.Advantages of Feature Hashing

1. **Scalability:** Handles very high-cardinality features efficiently (for example 16000 different categories).
2. **Fixed Output Size:** Output vector size is independent of the number of unique categories (defined by `n_features`).
3. **Memory Efficiency:** Reduces memory usage compared to one-hot encoding.
4. **Speed:** Fast to compute, suitable for online learning and streaming data.

.Disadvantages of Feature Hashing

1. **Hash Collisions:** Different categories may map to the same bucket, causing information loss.
2. **Non-Interpretability:** The meaning of each hashed feature is not human-interpretable.
3. **Irreversibility:** Cannot recover the original category from the hashed value.
4. **Potential for Reduced Accuracy:** Collisions can degrade model performance, especially with too few buckets.

.When to Use Feature Hashing

* When dealing with extremely high-cardinality categorical features (e.g., user IDs, URLs, words in text).
* When memory or computational efficiency is critical.
* In online learning or streaming scenarios where new categories may appear frequently.
* When interpretability of individual features is not required.

.When NOT to Use Feature Hashing

* When the number of unique categories is small (use one-hot encoding instead).
* When feature interpretability is important.
* When hash collisions would significantly impact model accuracy.
* For ordinal features where order matters.

**Summary:**  
Feature hashing is a powerful tool for efficiently encoding high-cardinality categorical variables, but it trades off interpretability and may introduce information loss due to collisions. Choose the number of buckets carefully to balance efficiency and



=== Bin Counting

Bin counting is a simple and efficient technique for encoding categorical variables, especially when the categories are already represented as non-negative integers (e.g., class labels, IDs). Instead of creating multiple columns (as in one-hot encoding), bin counting uses a single integer column to represent each category.

.What is Bin Counting?
Bin counting assigns each unique category an integer value (if not already present). The data is then represented as an array of these integer codes. This approach is memory-efficient and particularly useful for algorithms that can natively handle categorical features.

.How it works
1. Map each category to a unique integer (if not already integer-coded).
2. Store the integer codes in a single column.
3. Optionally, use `np.bincount` or similar functions to count occurrences of each category.

[source,python]
----
import numpy as np

# Example: category codes for each sample
categories = np.array([1, 2, 2, 0, 1, 3, 1, 2])

# Bin counting: counts occurrences of each category
counts = np.bincount(categories)
print(counts)  # Output: [1 3 3 1]
----

If your categories are strings, you can convert them to integers first:

[source,python]
----
from sklearn.preprocessing import LabelEncoder

categories = np.array(['cat', 'dog', 'dog', 'mouse', 'cat', 'cat'])
le = LabelEncoder()
int_categories = le.fit_transform(categories)
print(int_categories)  # e.g., [0 1 1 2 0 0]

counts = np.bincount(int_categories)
print(counts)  # Output: [3 2 1]
----

.Advantages of Bin Counting

1. **Memory Efficiency:** Uses a single column, saving memory compared to one-hot encoding.
2. **Speed:** Fast to compute and easy to implement.
3. **Suitable for High Cardinality:** Handles features with many categories efficiently.
4. **Direct Use in Some Models:** Decision trees and some gradient boosting frameworks (like CatBoost) can use


=== Odds Ratio Encoding

.What is Odds Ratio Encoding?

Odds Ratio Encoding is a categorical encoding technique that transforms categorical variables into numerical values based on the odds ratio of the target variable for each category. The odds ratio measures the likelihood of a particular outcome (e.g., target = 1) occurring in one category compared to the likelihood of it occurring in all other categories. This encoding is particularly useful in binary classification problems.

.How it Works

1. For each category in the categorical feature:
   * Calculate the odds of the target being 1 for that category:
     odds = (number of target=1 in category) / (number of target=0 in category)
   * Calculate the odds of the target being 1 for all other categories combined.
   * Compute the odds ratio:
     odds_ratio = odds_in_category / odds_in_other_categories
2. Optionally, take the logarithm of the odds ratio to reduce skewness and handle large values.
3. Replace each category with its corresponding (log) odds ratio.

.Advantages

* Captures the relationship between the categorical feature and the target variable.
* Useful for high-cardinality categorical features.
* Can improve model performance in binary classification tasks.
* Reduces dimensionality compared to one-hot encoding.

.Disadvantages

* Can lead to overfitting if categories have few samples (rare categories).
* Not suitable for features with no relationship to the target.
* Requires target variable, so cannot be used in unsupervised settings.
* Sensitive to data leakage if not applied correctly (should be fitted only on training data).

.When to Use

* When you have categorical features with many unique values (high cardinality).
* When the categorical feature is strongly related to the target variable.
* In binary classification problems.

.When Not to Use

* When the categorical feature has little or no relationship with the target.
* In unsupervised learning tasks.
* When categories have very few samples (risk of overfitting).
* In multi-class classification (unless adapted appropriately).

.Example

[source,python]
----
import pandas as pd
import numpy as np

def odds_ratio_encoding(df, col, target):
    odds_ratios = {}
    for category in df[col].unique():
        mask = df[col] == category
        odds_cat = (df[mask][target] == 1).sum() / max((df[mask][target] == 0).sum(), 1)
        odds_other = (df[~mask][target] == 1).sum() / max((df[~mask][target] == 0).sum(), 1)
        odds_ratio = odds_cat / odds_other if odds_other != 0 else np.nan
        odds_ratios[category] = np.log(odds_ratio) if odds_ratio > 0 else 0
    return df[col].map(odds_ratios)

# Example usage:
df['feature_encoded'] = odds_ratio_encoding(df, 'feature', 'target')
----


=== Feature Cross

.What is Feature Cross?

Feature cross is a technique used in machine learning to create new features by combining two or more existing categorical features. This process generates interaction features that can capture relationships between variables that may not be apparent when considering each feature independently. Feature crossing is especially useful for models that do not inherently capture feature interactions, such as linear models.

.How Does Feature Cross Work?

Feature crossing works by concatenating the values of two or more categorical features to form a new, composite feature. For example, if you have two features, `color` (with values like "red", "blue") and `shape` (with values like "circle", "square"), a feature cross would create new values such as "red_circle", "blue_square", etc.

This can be implemented in code by simply joining the string representations of the feature values, or by using specialized functions in libraries such as TensorFlow's `tf.feature_column.crossed_column` or scikit-learn's `ColumnTransformer` with custom transformers.

.Advantages

- *Captures interactions*: Allows models to learn from the interaction between features, which can improve predictive performance.
- *Improves linear models*: Enables linear models to approximate non-linear relationships by introducing interaction terms.
- *Flexible*: Can be applied to any combination of categorical features.

.Disadvantages

- *High cardinality*: Crossing features can lead to a large number of unique values, increasing the dimensionality of the data and potentially causing sparsity.
- *Overfitting*: With many crossed features, the model may overfit to the training data, especially if the dataset is not large enough.
- *Computational cost*: Increased number of features can lead to higher memory usage and slower training times.

.When to Use

- When you suspect that the interaction between two or more categorical features is important for prediction.
- When using models that do not automatically capture feature interactions (e.g., linear regression, logistic regression).
- When the number of unique combinations (cardinality) is manageable.

.When Not to Use

- When the crossed features result in extremely high cardinality, leading to computational inefficiency or overfitting.
- When using models that already capture feature interactions automatically (e.g., decision trees, random forests, gradient boosting machines).
- When there is insufficient data to support learning from the crossed features.

.Example

Suppose you have two categorical features: `city` (with values "London", "Paris") and `device` (with values "mobile", "desktop"). A feature cross would create new features like "London_mobile", "Paris_desktop", etc.

[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'city': ['London', 'Paris', 'London', 'Paris'],
    'device': ['mobile', 'desktop', 'desktop', 'mobile']
})

df['city_device'] = df['city'] + '_' + df['device']
print(df)
----

.Output:
----
     city   device    city_device
0  London   mobile   London_mobile
1   Paris  desktop   Paris_desktop
2  London  desktop   London_desktop
3   Paris   mobile   Paris_mobile
----

.Summary

Feature crossing is a powerful technique for enhancing categorical feature representation, especially in linear models. However, it should be used judiciously to avoid issues with high cardinality and overfitting.