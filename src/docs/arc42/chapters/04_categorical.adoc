:jbake-title: Categorical Variable
:jbake-type: page_toc
:jbake-status: published
:jbake-menu: arc42
:jbake-order: 4
:filename: /chapters/04_categorical.adoc
ifndef::imagesdir[:imagesdir: ../../images]

:toc:



[[section-solution-strategy]]
== Categorical Variable
Categorical variables describe qualitative phenomena - they represent characteristics, qualities, or group memberships rather than measurable quantities. This is the fundamental distinction from quantitative variables.

Quantitative Variables::
Quantitative Variables measure amounts
- Stock price: $145.32
- Temperature: 25°C
- Height: 175 cm
- Age: 32 years

Categorical Variables::
- Industry sector: "Technology", "Healthcare", "Finance"
- Customer satisfaction: "Poor", "Good", "Excellent"
- Country of origin: "USA", "Germany", "Japan"


.Type of Categorical Variables
1. Ordered (Ordinal) Categories - have a meaningful sequence:
- Education level: "High School" < "Bachelor's" < "Master's" < "PhD"
- Customer rating: "Poor" < "Fair" < "Good" < "Excellent"
- T-shirt sizes: "Small" < "Medium" < "Large" < "X-Large"

2. Unordered (Nominal) Categories - no inherent order:
- Cuisine type: "French", "Indian", "American", "Italian"
- Blood type: "A", "B", "AB", "O"
- Car manufacturer: "Toyota", "Ford", "BMW"


.Numbers can be categorical
Categorical variables are independent from data type - even numerical values can represent categories when they don't have mathematical meaning.

Example:
**Postal Codes:** "64295" (Darmstadt) and "55118" (Mainz)
(Darmstadt is not 9177 better than Mainz)
These numbers are just labels; mathematical operations don't make sense

**Other Numerical Categories:**

- Employee ID: 10234, 10235, 10236 (not meaningful to add or average)
- Product codes: 1001, 2005, 3010
- Phone area codes: 212, 415, 617
- Jersey numbers in sports: 23, 45, 99


.Machine Learning models for categorical data
- Decision trees can use categorical values as cut-offs
- CatBoost got its name from efficiently dealing with categorical variables
- Naive Bayes models work with contingency tables based on categorical values
**However** most models expect numerical data

=== One Hot Encoding

Why One Hot Encoding improves machine learning performance:
One-hot encoding is a crucial step in data preparation for machine learning algorithms. It involves converting categorical data into a numerical format that algorithms can understand. This transformation is essential because most machine learning models require numerical input to perform calculations and make predictions.

.One-hot encoding for categorical variables
Categorical data, by its nature, cannot be directly used by machine learning algorithms. These algorithms are designed to work with numerical data, and categorical variables need to be transformed into a format that these algorithms can process. Assigning arbitrary numerical values to to categorical data can lead to incorrect interpretations by the algorithm, as these values may imply a false order or hierarchy among the categories.  For instance, if we assign the values 0, 1, and 2 to the categories "UK", "French", and "US", respectively, the algorithm may incorrectly assume that "US" is twice as significant as "UK" or that "French" is midway between "UK" and "US".

.How it works
For each unique category in the original variable:
1. Create a new binary column
2. Set value to 1 where that category is present
3. Set value to 0 where that category is absent

[source,python]
----
# Example transformation
Original:          One-Hot Encoded:
City               DA  FFM  MZ
----               -------------
DA        →        1   0    0
FFM       →        0   1    0
MZ        →        0   0    1
DA        →        1   0    0
----

Each category gets its own column, and the presence of that category is indicated by a 1, while absence is indicated by a 0. This allows machine learning algorithms to treat each category independently without implying any order or hierarchy.

.Implementation in python using sklearn
[source,python]
----
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# Create sample data
data = pd.DataFrame({
    'city': ['New York', 'London', 'Paris', 'New York', 'London']
})

# Initialize encoder
encoder = OneHotEncoder(sparse_output=False)

# Fit and transform
encoded = encoder.fit_transform(data[['city']])

# Get feature names
feature_names = encoder.get_feature_names_out(['city'])
encoded_df = pd.DataFrame(encoded, columns=feature_names)
----

.Using pandas get_dummies
[source,python]
----
# Simple one-hot encoding with pandas
encoded = pd.get_dummies(data, columns=['city'])

# With prefix
encoded = pd.get_dummies(data, columns=['city'], prefix='city')
----


.Advantages of One-Hot Encoding

1. Prevents models from assuming false relationships between categories
* Example: Without encoding, a model might think "City_ID 3" > "City_ID 1"

2. **Model Compatibility:** 
* Most ML algorithms require numerical input
* Enables use of linear models, neural networks, SVMs, etc.

3. **Clear Representation**
* Each category is independently represented
* Easy to interpret which category is active

4. **Handles Non-Ordinal Categories**
* Perfect for nominal variables with no natural order
* Examples: Colors, cities, product types



.Disadvantages of One-Hot Encoding

1. **Curse of Dimensionality**: WARNING: High cardinality features can explode feature space.
1000 unique cities → 1,000 new columns
10000 product IDs → 10,000 new columns


2. **Sparse Data**
* Most values are 0 (only one 1 per row)
* Increased memory usage
* Computational inefficiency


3. **Multicollinearity**
* Columns are linearly dependent: sum of all columns = 1
* Can cause numerical instability in some algorithms
* Solution: Drop one column (dummy coding)


4. **Incomplete Vocabulary Problem** 
[source,python]
----
# Training data sees: ['NYC', 'LA', 'Chicago']
# Test data has: ['NYC', 'LA', 'Boston']  # Boston causes error!

# Solution: Use handle_unknown='ignore'
encoder = OneHotEncoder(handle_unknown='ignore')
----

.When to Use One-Hot Encoding
1. **Low to Medium Cardinality** (<100 unique values) 
2. **Nominal Categories** (no natural order)
   * Colors: Red, Blue, Green
   * Countries: USA, UK, Japan
   * Product categories: Electronics, Clothing, Food

3. **Stable Categories** (unlikely to see new values)
4. **Model Requires Numerical Input**
   * Linear/Logistic Regression
   * Neural Networks
   * Support Vector Machines

5. **Interpretability Matters**
   * Each coefficient represents one category's effect

**Example Use Cases:**
[source,python]
----
# Good candidates for one-hot encoding
df['day_of_week']     # 7 values: Mon-Sun
df['region']          # 5 values: North, South, East, West, Central  
df['product_type']    # 10 values: known product categories
df['education_level'] # 6 values: HS, Bachelor's, Master's, etc.
----


.❌ Avoid When:

1. **High Cardinality** (>1000 unique values) [[6]]
   * User IDs, Device IDs
   * ZIP codes, IP addresses
   * URLs, email domains

2. **Ordinal Categories** (natural order exists)
   * Ratings: Poor, Fair, Good, Excellent
   * Sizes: S, M, L, XL
   * Education levels (if order matters)

3. **Text Data** (use embeddings instead)
4. **Continuous Variables** (use scaling/binning)
5. **Memory Constraints** (sparse matrices become too large)


=== Feature Hashing

Feature hashing, also known as the "hashing trick", is a technique for converting categorical variables (especially those with high cardinality) into a fixed-length numerical vector using a hash function. It is widely used in large-scale machine learning and natural language processing tasks.

.What is Feature Hashing?
Feature hashing maps each category (or token) to one of a fixed number of columns (buckets) using a hash function. Instead of creating a column for every unique category (as in one-hot encoding), all categories are distributed across a predefined number of columns. This approach is highly memory-efficient and scalable.

.How it works
1. Choose the number of output features (buckets), e.g., 1024.
2. For each category value, compute its hash and map it to a bucket index.
3. If multiple categories hash to the same bucket (collision), their values are combined (typically summed).
4. The resulting feature vector has a fixed length, regardless of the number of unique categories.

[source,python]
----
from sklearn.feature_extraction import FeatureHasher

# Example: Hashing city names into 4 buckets
data = [{'city': 'New York'}, {'city': 'London'}, {'city': 'Paris'}, {'city': 'New York'}]
hasher = FeatureHasher(n_features=4, input_type='dict')
hashed_features = hasher.transform(data)
print(hashed_features.toarray())
----

.Advantages of Feature Hashing

1. **Scalability:** Handles very high-cardinality features efficiently.
2. **Fixed Output Size:** Output vector size is independent of the number of unique categories.
3. **Memory Efficiency:** Reduces memory usage compared to one-hot encoding.
4. **Speed:** Fast to compute, suitable for online learning and streaming data.

.Disadvantages of Feature Hashing

1. **Hash Collisions:** Different categories may map to the same bucket, causing information loss.
2. **Non-Interpretability:** The meaning of each hashed feature is not human-interpretable.
3. **Irreversibility:** Cannot recover the original category from the hashed value.
4. **Potential for Reduced Accuracy:** Collisions can degrade model performance, especially with too few buckets.

.When to Use Feature Hashing

* When dealing with extremely high-cardinality categorical features (e.g., user IDs, URLs, words in text).
* When memory or computational efficiency is critical.
* In online learning or streaming scenarios where new categories may appear frequently.
* When interpretability of individual features is not required.

.When NOT to Use Feature Hashing

* When the number of unique categories is small (use one-hot encoding instead).
* When feature interpretability is important.
* When hash collisions would significantly impact model accuracy.
* For ordinal features where order matters.

**Summary:**  
Feature hashing is a powerful tool for efficiently encoding high-cardinality categorical variables, but it trades off interpretability and may introduce information loss due to collisions. Choose the number of buckets carefully to balance efficiency and



=== Bin Counting

Bin counting is a simple and efficient technique for encoding categorical variables, especially when the categories are already represented as non-negative integers (e.g., class labels, IDs). Instead of creating multiple columns (as in one-hot encoding), bin counting uses a single integer column to represent each category.

.What is Bin Counting?
Bin counting assigns each unique category an integer value (if not already present). The data is then represented as an array of these integer codes. This approach is memory-efficient and particularly useful for algorithms that can natively handle categorical features.

.How it works
1. Map each category to a unique integer (if not already integer-coded).
2. Store the integer codes in a single column.
3. Optionally, use `np.bincount` or similar functions to count occurrences of each category.

[source,python]
----
import numpy as np

# Example: category codes for each sample
categories = np.array([1, 2, 2, 0, 1, 3, 1, 2])

# Bin counting: counts occurrences of each category
counts = np.bincount(categories)
print(counts)  # Output: [1 3 3 1]
----

If your categories are strings, you can convert them to integers first:

[source,python]
----
from sklearn.preprocessing import LabelEncoder

categories = np.array(['cat', 'dog', 'dog', 'mouse', 'cat', 'cat'])
le = LabelEncoder()
int_categories = le.fit_transform(categories)
print(int_categories)  # e.g., [0 1 1 2 0 0]

counts = np.bincount(int_categories)
print(counts)  # Output: [3 2 1]
----

.Advantages of Bin Counting

1. **Memory Efficiency:** Uses a single column, saving memory compared to one-hot encoding.
2. **Speed:** Fast to compute and easy to implement.
3. **Suitable for High Cardinality:** Handles features with many categories efficiently.
4. **Direct Use in Some Models:** Decision trees and some gradient boosting frameworks (like CatBoost) can use


=== Odds Ratio Encoding

.What is Odds Ratio Encoding?

Odds Ratio Encoding is a categorical encoding technique that transforms categorical variables into numerical values based on the odds ratio of the target variable for each category. The odds ratio measures the likelihood of a particular outcome (e.g., target = 1) occurring in one category compared to the likelihood of it occurring in all other categories. This encoding is particularly useful in binary classification problems.

.How it Works

1. For each category in the categorical feature:
   * Calculate the odds of the target being 1 for that category:
     odds = (number of target=1 in category) / (number of target=0 in category)
   * Calculate the odds of the target being 1 for all other categories combined.
   * Compute the odds ratio:
     odds_ratio = odds_in_category / odds_in_other_categories
2. Optionally, take the logarithm of the odds ratio to reduce skewness and handle large values.
3. Replace each category with its corresponding (log) odds ratio.

.Advantages

* Captures the relationship between the categorical feature and the target variable.
* Useful for high-cardinality categorical features.
* Can improve model performance in binary classification tasks.
* Reduces dimensionality compared to one-hot encoding.

.Disadvantages

* Can lead to overfitting if categories have few samples (rare categories).
* Not suitable for features with no relationship to the target.
* Requires target variable, so cannot be used in unsupervised settings.
* Sensitive to data leakage if not applied correctly (should be fitted only on training data).

.When to Use

* When you have categorical features with many unique values (high cardinality).
* When the categorical feature is strongly related to the target variable.
* In binary classification problems.

.When Not to Use

* When the categorical feature has little or no relationship with the target.
* In unsupervised learning tasks.
* When categories have very few samples (risk of overfitting).
* In multi-class classification (unless adapted appropriately).

.Example

[source,python]
----
import pandas as pd
import numpy as np

def odds_ratio_encoding(df, col, target):
    odds_ratios = {}
    for category in df[col].unique():
        mask = df[col] == category
        odds_cat = (df[mask][target] == 1).sum() / max((df[mask][target] == 0).sum(), 1)
        odds_other = (df[~mask][target] == 1).sum() / max((df[~mask][target] == 0).sum(), 1)
        odds_ratio = odds_cat / odds_other if odds_other != 0 else np.nan
        odds_ratios[category] = np.log(odds_ratio) if odds_ratio > 0 else 0
    return df[col].map(odds_ratios)

# Example usage:
df['feature_encoded'] = odds_ratio_encoding(df, 'feature', 'target')
----