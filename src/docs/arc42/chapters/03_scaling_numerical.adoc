:jbake-title: Scaling Numerical
:jbake-type: page_toc
:jbake-status: published
:jbake-menu: arc42
:jbake-order: 3
:filename: /chapters/03_scaling_numerical.adoc
ifndef::imagesdir[:imagesdir: ../../images]

:toc:



[[section-context-and-scope]]
== Scaling Numerical
This document defines and illustrates concepts around Scaling and Normalizing. Use it as a quick reference when preparing numeric data for machine-learning pipelines.

.Categorical Data and Numerical Data
There are two types of data

Categorical Data:: Data that reflect qualitative characteristics
* Gender
* Hair Color
* Ethnicity
utilising categorical data is essentially a way of assigning number values to inherently qualitative data. For example you can assign Female to 1 and Male to 2
Numerical Data:: Data that are natureally numbers-based, for example age, height and weight....

Within each of these main data types , there are 2 level of measurement

.For categorical data it is

Nominal Data:: Pure identifier, swapping two labels never changes meaning
* *Valid operations -* = ≠, counting frequencies, one‐hot/dummy/feature-hash encodings.
* *Invalid operations –* <, >, +, −, computing mean or standard deviation.
* *Example*: Eye Colour: brown, blue ,green or Country Code DE, FR, US
Ordinal Data:: * Categories imply rank, but the gap between successive ranks is unknown or unequal.
* *Valid operations –* =, ≠, <, >, non-parametric statistics (median, Spearman ρ, Kendall τ).
* *Invalid operations –* Adding / subtracting ranks as if they were numeric distances (unless you trust equal spacing).
* *Example*: high-school < bachelor < master < PhD or strongly disagree (1) … strongly agree (5)

.For numerical data it is 
Interval:: Numeric values are ordered and evenly spaced.
* Zero is arbitrary: it does not mean “none of the quantity.”
* Because zero is arbitrary, ratios are meaningless; you may add or subtract, but not multiply or divide and still retain semantic truth.
* *Example*:  
** Temperature in °C or °F (0 °C is not “no temperature”).
**  Calendar dates measured as “years” (the year 0 is a convention).
** Standardised test scores (e.g., GMAT 200-800).
Ratio data:: Ordered, evenly spaced and the scale has an absolute, natural zero that truly represents “absence.”
* All arithmetic operations—including multiplication, division, percentage change—retain meaning.
* *Example*: 
** Height, weight, length, price, age, count, elapsed time, Kelvin temperature.
** Sensor readings such as luminosity in lux, sound pressure in pascals, CPU cycles.

.Why Scale? Machine-learning optimizer behave best when every input feature has roughly the same dynamic range.
Unscaled data can lead to: 

* divergent or sluggish gradient descent.
* domination of distance metrics by large-magnitude features
* ppor floating-point precision

Most scaling method aim to map feature around *[-1 , 1]*, or to a zero-mean/unit-variance bell shape

=== Linear Scaling Methods

==== Min-Max Scaling
[source,python]
----
x_scaled = (2*x - x_max - x_min)/(x_max - x_min) # maps to [-1, 1]
----

pros:: deterministic bounds, simple inverse transform.
cons:: extremely sensitive to outliers because *x_min/x_max* might be rare anomalies.

.Task Overview
[source, python]
----
'''
Your task: 
    - Open the starter notebook and load housing data
    - Apply a min max scaler to the column total_rooms
    - Verify the transformed column is in range [-1,1]
'''
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# -------------------------------------------------------------
# 1. read the file
# -------------------------------------------------------------
df = pd.read_csv("housing.csv")

# -------------------------------------------------------------
# 2. scale `total_rooms`
# -------------------------------------------------------------
scaler = MinMaxScaler(feature_range=(-1, 1))
df["total_rooms_scaled"] = scaler.fit_transform(df[["total_rooms"]])

#Verify max and min total_room
max_rooms = df["total_rooms"].max()
min_rooms = df["total_rooms"].min()
print("max total rooms: " + str(max_rooms))
print("min total rooms: " + str(min_rooms))

# -------------------------------------------------------------
# 3. write the result
# -------------------------------------------------------------
df.to_csv("housing_scaled.csv", index=False)   # <- new file with the extra column
print("✅  new file written: housing_scaled.csv")
----

With the result output, we can determine the maximum and minimum total_rooms
max total rooms:: 39320.0
min total rooms:: 2.0

Taking a random total_rooms of *880.0*
----
x_scaled = (2*880 - 39320 - 2 ) / (32320 - 2) = -0.95553385
----
Which is a correct scaled number in range [-1, 1]

==== Clipping + Min-Max
Clipping with Min-Max scaling is a data preprocessing technique that combines outlier handling with feature scaling. This approach addresses the limitations of standard Min-Max scaling when dealing with datasets containing extreme values or outliers.

.Clipping (Clamping)
Clipping, also known as clamping, is the process of limiting values to a specific range by setting a minimum and maxium threshold. Unlike wrapping, clipping moves values outside the range to the nearest boundary value
[source,text]
----
Original value: 150
Clipping range: [0, 100]
Clipped value: 100

Original value: -20
Clipping range: [0, 100]
Clipped value: 0
----
.Min-Max Scaling (mentioned in last section)
Min-Max scaling is a normalization technique that transforms features to a fixed range, typically [0, 1] or [-1, 1].

.The two-step process 
The clipping with Min-Max scaling approach involves:

1. **First**: Clip the data using reasonable bounds (not the actual min/max)
2. **Then**: Apply Min-Max scaling to transform the clipped data to [-1, 1]

This differs from `sklearn.preprocessing.MinMaxScaler(clip=True)`, which scales first and then clips.

.How it works
* *np.clip(x, lo, hi)* caps insane values.
* Run classic min-max on the clipped data. You keep genuine outliers (they become -1 or +1) while protecting the bulk of the distribution from compression. NOTE: this is *not* the same as *MinMaxScaler(clip=True)* in scikit-learn, which only clips after scaling.
image:clipping_min_max.png[]



Advantages::
1. **Outlier Handling**: Extreme values are treated as boundary cases rather than dominating the scaling
2. **Preserves Data Distribution**: The majority of the data maintains its relative relationships
3. **Bounded Output**: Guarantees all values fall within [-1, 1] range
4. **Robustness**: Less sensitive to extreme outliers compared to standard Min-Max scaling
Disadvantages::
1. **Information Loss**: Outliers lose their relative magnitude
2. **Parameter Selection**: Requires choosing appropriate clipping bounds
3. **Domain Knowledge**: May need understanding of what constitutes "reasonable" values

.When to use?
This technique is particularly useful when:

* Your dataset contains outliers that shouldn't dominate the scaling
* You need values strictly bounded to a specific range
* The outliers represent valid but extreme cases (not errors)
* You're working with algorithms sensitive to feature scales (e.g., neural networks, SVM)



.Conclusion
Clipping with Min-Max scaling provides a robust approach to feature scaling when dealing with outliers. By setting reasonable bounds before scaling, we ensure that extreme values don't compress the majority of our data into a small range, while still maintaining the desired output range of [-1, 1].






==== Z-Score Normalization
Z-score normalization, also known as standardization or standard scaling, is a fundamental data preprocessing technique in machine learning and statistics. It transforms data to have a mean of zero and a standard deviation of one, making different features comparable regardless of their original scales.

.What is Z-Score Normalization?

Z-score normalization is a scaling technique that transforms each data point by subtracting the mean and dividing by the standard deviation. This process centers the data around zero and scales it based on the data's spread.


.The Z-Score Formula

The mathematical formula for Z-score normalization is:

[source,text]
----
z = (x - μ) / σ

Where:
- z = standardized value (Z-score)
- x = original value
- μ = mean of the dataset
- σ = standard deviation of the dataset
----


.Key Properties

After Z-score normalization:

* **Mean = 0**: The transformed data is centered at zero
* **Standard Deviation = 1**: The data has unit variance
* **Range**: Typically between -3 and +3, but theoretically unbounded
* **Distribution shape**: Preserved from the original data


.Step-by-Step Process

1. **Calculate the mean** of the dataset
2. **Calculate the standard deviation** of the dataset
3. **For each data point**, subtract the mean and divide by the standard deviation

.Visual Interpretation

* **Positive Z-scores**: Values above the mean
* **Negative Z-scores**: Values below the mean
* **Magnitude**: Indicates how many standard deviations away from the mean
image:z_score_housing.png[]


Advantages of Z-Score Normalization::
1. **Scale Independence**: Makes features with different units comparable
2. **Outlier Identification**: Values beyond ±3 standard deviations are easily identified as outliers
3. **Algorithm Performance**: Many ML algorithms perform better with standardized features
4. **Statistical Properties**: Preserves the shape of the original distribution
5. **Interpretability**: Z-scores have a clear statistical meaning

Disadvantages and Limitations::
1. **Unbounded Range**: Unlike min-max scaling, Z-scores are not bounded to a specific range
2. **Assumes Normal Distribution**: Works best when data is approximately normally distributed
3. **Sensitive to Outliers**: Extreme outliers can affect the mean and standard deviation
4. **Not Suitable for All Algorithms**: Some algorithms prefer bounded inputs (e.g., neural networks with sigmoid activation)



.When to Use Z-Score Normalization

Z-score normalization is particularly effective when:

* Working with algorithms that assume normally distributed data (e.g., Linear Regression, Logistic Regression)
* Features have different units or scales
* You need to identify outliers based on statistical thresholds
* Using distance-based algorithms (e.g., k-NN, k-Means, SVM)
* The data approximately follows a normal distribution



.Conclusion

Z-score normalization is a powerful preprocessing technique that standardizes features to have zero mean and unit variance. It's particularly valuable for:

* Making features with different scales comparable
* Improving the performance of many machine learning algorithms
* Identifying statistical outliers
* Ensuring numerical stability in optimization algorithms

While it has limitations (unbounded range, sensitivity to outliers), Z-score normalization remains one of the most widely used and effective scaling techniques in data science and machine learning.