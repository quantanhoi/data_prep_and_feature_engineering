:jbake-title: Scaling Numerical
:jbake-type: page_toc
:jbake-status: published
:jbake-menu: arc42
:jbake-order: 3
:filename: /chapters/03_scaling_numerical.adoc
ifndef::imagesdir[:imagesdir: ../../images]

:toc:



[[section-context-and-scope]]
== Scaling Numerical
This document defines and illustrates concepts around Scaling and Normalizing. Use it as a quick reference when preparing numeric data for machine-learning pipelines.

.Categorical Data and Numerical Data
There are two types of data

Categorical Data:: Data that reflect qualitative characteristics
* Gender
* Hair Color
* Ethnicity
utilising categorical data is essentially a way of assigning number values to inherently qualitative data. For example you can assign Female to 1 and Male to 2
Numerical Data:: Data that are natureally numbers-based, for example age, height and weight....

Within each of these main data types , there are 2 level of measurement

.For categorical data it is

Nominal Data:: Pure identifier, swapping two labels never changes meaning
* *Valid operations -* = ≠, counting frequencies, one‐hot/dummy/feature-hash encodings.
* *Invalid operations –* <, >, +, −, computing mean or standard deviation.
* *Example*: Eye Colour: brown, blue ,green or Country Code DE, FR, US
Ordinal Data:: * Categories imply rank, but the gap between successive ranks is unknown or unequal.
* *Valid operations –* =, ≠, <, >, non-parametric statistics (median, Spearman ρ, Kendall τ).
* *Invalid operations –* Adding / subtracting ranks as if they were numeric distances (unless you trust equal spacing).
* *Example*: high-school < bachelor < master < PhD or strongly disagree (1) … strongly agree (5)

.For numerical data it is 
Interval:: Numeric values are ordered and evenly spaced.
* Zero is arbitrary: it does not mean “none of the quantity.”
* Because zero is arbitrary, ratios are meaningless; you may add or subtract, but not multiply or divide and still retain semantic truth.
* *Example*:  
** Temperature in °C or °F (0 °C is not “no temperature”).
**  Calendar dates measured as “years” (the year 0 is a convention).
** Standardised test scores (e.g., GMAT 200-800).
Ratio data:: Ordered, evenly spaced and the scale has an absolute, natural zero that truly represents “absence.”
* All arithmetic operations—including multiplication, division, percentage change—retain meaning.
* *Example*: 
** Height, weight, length, price, age, count, elapsed time, Kelvin temperature.
** Sensor readings such as luminosity in lux, sound pressure in pascals, CPU cycles.

.Why Scale? Machine-learning optimizer behave best when every input feature has roughly the same dynamic range.
Unscaled data can lead to: 

* divergent or sluggish gradient descent.
* domination of distance metrics by large-magnitude features
* ppor floating-point precision

Most scaling method aim to map feature around *[-1 , 1]*, or to a zero-mean/unit-variance bell shape

=== Linear Scaling Methods

==== Min-Max Scaling
[source,python]
----
x_scaled = (2*x - x_max - x_min)/(x_max - x_min) # maps to [-1, 1]
----

pros:: deterministic bounds, simple inverse transform.
cons:: extremely sensitive to outliers because *x_min/x_max* might be rare anomalies.

.Task Overview
[source, python]
----
'''
Your task: 
    - Open the starter notebook and load housing data
    - Apply a min max scaler to the column total_rooms
    - Verify the transformed column is in range [-1,1]
'''
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# -------------------------------------------------------------
# 1. read the file
# -------------------------------------------------------------
df = pd.read_csv("housing.csv")

# -------------------------------------------------------------
# 2. scale `total_rooms`
# -------------------------------------------------------------
scaler = MinMaxScaler(feature_range=(-1, 1))
df["total_rooms_scaled"] = scaler.fit_transform(df[["total_rooms"]])

#Verify max and min total_room
max_rooms = df["total_rooms"].max()
min_rooms = df["total_rooms"].min()
print("max total rooms: " + str(max_rooms))
print("min total rooms: " + str(min_rooms))

# -------------------------------------------------------------
# 3. write the result
# -------------------------------------------------------------
df.to_csv("housing_scaled.csv", index=False)   # <- new file with the extra column
print("✅  new file written: housing_scaled.csv")
----

With the result output, we can determine the maximum and minimum total_rooms
max total rooms:: 39320.0
min total rooms:: 2.0

Taking a random total_rooms of *880.0*
----
x_scaled = (2*880 - 39320 - 2 ) / (32320 - 2) = -0.95553385
----
Which is a correct scaled number in range [-1, 1]

==== Clipping + Min-Max
Clipping with Min-Max scaling is a data preprocessing technique that combines outlier handling with feature scaling. This approach addresses the limitations of standard Min-Max scaling when dealing with datasets containing extreme values or outliers.

.Clipping (Clamping)
Clipping, also known as clamping, is the process of limiting values to a specific range by setting a minimum and maxium threshold. Unlike wrapping, clipping moves values outside the range to the nearest boundary value
[source,text]
----
Original value: 150
Clipping range: [0, 100]
Clipped value: 100

Original value: -20
Clipping range: [0, 100]
Clipped value: 0
----
.Min-Max Scaling (mentioned in last section)
Min-Max scaling is a normalization technique that transforms features to a fixed range, typically [0, 1] or [-1, 1].

.The two-step process 
The clipping with Min-Max scaling approach involves:

1. **First**: Clip the data using reasonable bounds (not the actual min/max)
2. **Then**: Apply Min-Max scaling to transform the clipped data to [-1, 1]

This differs from `sklearn.preprocessing.MinMaxScaler(clip=True)`, which scales first and then clips.

.How it works
* *np.clip(x, lo, hi)* caps insane values.
* Run classic min-max on the clipped data. You keep genuine outliers (they become -1 or +1) while protecting the bulk of the distribution from compression. NOTE: this is *not* the same as *MinMaxScaler(clip=True)* in scikit-learn, which only clips after scaling.
image:clipping_min_max.png[]



Advantages::
1. **Outlier Handling**: Extreme values are treated as boundary cases rather than dominating the scaling
2. **Preserves Data Distribution**: The majority of the data maintains its relative relationships
3. **Bounded Output**: Guarantees all values fall within [-1, 1] range
4. **Robustness**: Less sensitive to extreme outliers compared to standard Min-Max scaling
Disadvantages::
1. **Information Loss**: Outliers lose their relative magnitude
2. **Parameter Selection**: Requires choosing appropriate clipping bounds
3. **Domain Knowledge**: May need understanding of what constitutes "reasonable" values

.When to use?
This technique is particularly useful when:

* Your dataset contains outliers that shouldn't dominate the scaling
* You need values strictly bounded to a specific range
* The outliers represent valid but extreme cases (not errors)
* You're working with algorithms sensitive to feature scales (e.g., neural networks, SVM)



.Conclusion
Clipping with Min-Max scaling provides a robust approach to feature scaling when dealing with outliers. By setting reasonable bounds before scaling, we ensure that extreme values don't compress the majority of our data into a small range, while still maintaining the desired output range of [-1, 1].