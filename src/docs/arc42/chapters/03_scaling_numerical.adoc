:jbake-title: Scaling Numerical
:jbake-type: page_toc
:jbake-status: published
:jbake-menu: arc42
:jbake-order: 3
:filename: /chapters/03_scaling_numerical.adoc
ifndef::imagesdir[:imagesdir: ../../images]

:toc:



[[section-context-and-scope]]
== Scaling Numerical
This document defines and illustrates concepts around Scaling and Normalizing. Use it as a quick reference when preparing numeric data for machine-learning pipelines.

.Categorical Data vs Numerical Data
There are two types of data

Categorical Data:: Data that reflect qualitative characteristics
* Gender
* Hair Color
* Ethnicity
utilising categorical data is essentially a way of assigning number values to inherently qualitative data. For example you can assign Female to 1 and Male to 2
Numerical Data:: Data that are natureally numbers-based, for example age, height and weight....

Within each of these main data types , there are 2 level of measurement

.For categorical data it is

Nominal Data:: Pure identifier, swapping two labels never changes meaning
* *Valid operations -* = ≠, counting frequencies, one‐hot/dummy/feature-hash encodings.
* *Invalid operations –* <, >, +, −, computing mean or standard deviation.
* *Example*: Eye Colour: brown, blue ,green or Country Code DE, FR, US
Ordinal Data:: * Categories imply rank, but the gap between successive ranks is unknown or unequal.
* *Valid operations –* =, ≠, <, >, non-parametric statistics (median, Spearman ρ, Kendall τ).
* *Invalid operations –* Adding / subtracting ranks as if they were numeric distances (unless you trust equal spacing).
* *Example*: high-school < bachelor < master < PhD or strongly disagree (1) … strongly agree (5)

.For numerical data it is 
Interval:: Numeric values are ordered and evenly spaced.
* Zero is arbitrary: it does not mean “none of the quantity.”
* Because zero is arbitrary, ratios are meaningless; you may add or subtract, but not multiply or divide and still retain semantic truth.
* *Example*:  
** Temperature in °C or °F (0 °C is not “no temperature”).
**  Calendar dates measured as “years” (the year 0 is a convention).
** Standardised test scores (e.g., GMAT 200-800).
Ratio data:: Ordered, evenly spaced and the scale has an absolute, natural zero that truly represents “absence.”
* All arithmetic operations—including multiplication, division, percentage change—retain meaning.
* *Example*: 
** Height, weight, length, price, age, count, elapsed time, Kelvin temperature.
** Sensor readings such as luminosity in lux, sound pressure in pascals, CPU cycles.

.Why Scale? Machine-learning optimizer behave best when every input feature has roughly the same dynamic range.
Unscaled data can lead to: 

* divergent or sluggish gradient descent.
* domination of distance metrics by large-magnitude features
* ppor floating-point precision

Most scaling method aim to map feature around *[-1 , 1]*, or to a zero-mean/unit-variance bell shape

=== Linear Scaling Methods

==== Min-Max Scaling
[source,python]
----
x_scaled = (2*x - x_max - x_min)/(x_max - x_min) # maps to [-1, 1]
----

pros:: deterministic bounds, simple inverse transform.
cons:: extremely sensitive to outliers because *x_min/x_max* might be rare anomalies.

.Task Overview
[source, python]
----
'''
Your task: 
    - Open the starter notebook and load housing data
    - Apply a min max scaler to the column total_rooms
    - Verify the transformed column is in range [-1,1]
'''
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# -------------------------------------------------------------
# 1. read the file
# -------------------------------------------------------------
df = pd.read_csv("housing.csv")

# -------------------------------------------------------------
# 2. scale `total_rooms`
# -------------------------------------------------------------
scaler = MinMaxScaler(feature_range=(-1, 1))
df["total_rooms_scaled"] = scaler.fit_transform(df[["total_rooms"]])

#Verify max and min total_room
max_rooms = df["total_rooms"].max()
min_rooms = df["total_rooms"].min()
print("max total rooms: " + str(max_rooms))
print("min total rooms: " + str(min_rooms))

# -------------------------------------------------------------
# 3. write the result
# -------------------------------------------------------------
df.to_csv("housing_scaled.csv", index=False)   # <- new file with the extra column
print("✅  new file written: housing_scaled.csv")
----

With the result output, we can determine the maximum and minimum total_rooms
max total rooms:: 39320.0
min total rooms:: 2.0

Taking a random total_rooms of *880.0*
----
x_scaled = (2*880 - 39320 - 2 ) / (32320 - 2) = -0.95553385
----
Which is a correct scaled number in range [-1, 1]






==== Clipping + Min-Max
Clipping with Min-Max scaling is a data preprocessing technique that combines outlier handling with feature scaling. This approach addresses the limitations of standard Min-Max scaling when dealing with datasets containing extreme values or outliers.

.Clipping (Clamping)
Clipping, also known as clamping, is the process of limiting values to a specific range by setting a minimum and maxium threshold. Unlike wrapping, clipping moves values outside the range to the nearest boundary value
[source,text]
----
Original value: 150
Clipping range: [0, 100]
Clipped value: 100

Original value: -20
Clipping range: [0, 100]
Clipped value: 0
----
.Min-Max Scaling (mentioned in last section)
Min-Max scaling is a normalization technique that transforms features to a fixed range, typically [0, 1] or [-1, 1].

.The two-step process 
The clipping with Min-Max scaling approach involves:

1. **First**: Clip the data using reasonable bounds (not the actual min/max)
2. **Then**: Apply Min-Max scaling to transform the clipped data to [-1, 1]

This differs from `sklearn.preprocessing.MinMaxScaler(clip=True)`, which scales first and then clips.

.How it works
* *np.clip(x, lo, hi)* caps insane values.
* Run classic min-max on the clipped data. You keep genuine outliers (they become -1 or +1) while protecting the bulk of the distribution from compression. NOTE: this is *not* the same as *MinMaxScaler(clip=True)* in scikit-learn, which only clips after scaling.
image:clipping_min_max.png[]



Advantages::
1. **Outlier Handling**: Extreme values are treated as boundary cases rather than dominating the scaling
2. **Preserves Data Distribution**: The majority of the data maintains its relative relationships
3. **Bounded Output**: Guarantees all values fall within [-1, 1] range
4. **Robustness**: Less sensitive to extreme outliers compared to standard Min-Max scaling
Disadvantages::
1. **Information Loss**: Outliers lose their relative magnitude
2. **Parameter Selection**: Requires choosing appropriate clipping bounds
3. **Domain Knowledge**: May need understanding of what constitutes "reasonable" values

.When to use?
This technique is particularly useful when:

* Your dataset contains outliers that shouldn't dominate the scaling
* You need values strictly bounded to a specific range
* The outliers represent valid but extreme cases (not errors)
* You're working with algorithms sensitive to feature scales (e.g., neural networks, SVM)

.Example adding an extreme outlier then clipping
[source,python]
----
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
df = pd.read_csv('./housing.csv')
print(df.head(2))
----

.Output:
[source,python]
----
   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \
0    -122.23     37.88                41.0        880.0           129.0   
1    -122.22     37.86                21.0       7099.0          1106.0   

   population  households  median_income  median_house_value ocean_proximity  
0       322.0       126.0         8.3252            452600.0        NEAR BAY  
1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  
----

[source,python]
----
scaler = MinMaxScaler(feature_range=(-1, 1), clip=True) 
# Nothing changes because the training data define the min/max themselves, 
#so every scaled value is already inside [-1 , 1]
df['total_rooms_scaled'] = scaler.fit_transform(df[['total_rooms']])
print(df.head(2))
----
.Output:
[source,python]
----
   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \
0    -122.23     37.88                41.0        880.0           129.0   
1    -122.22     37.86                21.0       7099.0          1106.0   

   population  households  median_income  median_house_value ocean_proximity  \
0       322.0       126.0         8.3252            452600.0        NEAR BAY   
1      2401.0      1138.0         8.3014            358500.0        NEAR BAY   

   total_rooms_scaled  
0           -0.955339  
1           -0.638995  
----

[source,python]
----
# simulate a new observation with an outlier
new = pd.DataFrame({'total_rooms': [999_999]})
print(new.tail())
print("With clip=True :", scaler.transform(new))   # → exactly 1
scaler.clip = False                                # turn off
print("With clip=False:", scaler.transform(new))   # → >1, outside the range
----
.Output:
[source,python]
----
total_rooms
0       999999
With clip=True : [[1.]]
With clip=False: [[49.86713465]]
----
The output shows that with clipping enabled, the extreme value of 999,999 is transformed to exactly 1, while without clipping, it exceeds the range and becomes approximately 49.87. However, if the extreme value is scaled to 1 or -1, then the other values are compressed into a very small range, which is not desirable.


==== Clipping within the dataset using np.clip
.With np.clip you manually choose the reasonable lower/ upper bounds, then truncate them before any scaler see them
[source,python]
----
import numpy as np
# Clip total_rooms to be between 1,000 and 6000
df['total_rooms'] = np.clip(df['total_rooms'], 1000, 6000)
print(df['total_rooms'].describe())
----
.Output:
[source,python]
----
count    20640.000000
mean      2502.360514
std       1386.068526
min       1000.000000
25%       1447.750000
50%       2127.000000
75%       3148.000000
max       6000.000000
Name: total_rooms, dtype: float64
----
.Then you can apply the Min-Max scaler
[source,python]
----
scaler = MinMaxScaler(feature_range=(-1, 1), clip=True) 
# Nothing changes because the training data define the min/max themselves, 
#so every scaled value is already inside [-1 , 1]
df['total_rooms_scaled'] = scaler.fit_transform(df[['total_rooms']])
print(df.head(2))
----
.Output:
[source,python]
----
longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \
0    -122.23     37.88                41.0       1000.0           129.0   
1    -122.22     37.86                21.0       6000.0          1106.0   

   population  households  median_income  median_house_value ocean_proximity  \
0       322.0       126.0         8.3252            452600.0        NEAR BAY   
1      2401.0      1138.0         8.3014            358500.0        NEAR BAY   

   total_rooms_scaled  
0                -1.0  
1                 1.0
----

.Conclusion
Clipping with Min-Max scaling provides a robust approach to feature scaling when dealing with outliers. By setting reasonable bounds before scaling, we ensure that extreme values don't compress the majority of our data into a small range, while still maintaining the desired output range of [-1, 1].






==== Z-Score Normalization
Z-score normalization, also known as standardization or standard scaling, is a fundamental data preprocessing technique in machine learning and statistics. It transforms data to have a mean of zero and a standard deviation of one, making different features comparable regardless of their original scales.

.What is Z-Score Normalization?

Z-score normalization is a scaling technique that transforms each data point by subtracting the mean and dividing by the standard deviation. This process centers the data around zero and scales it based on the data's spread.


.The Z-Score Formula

The mathematical formula for Z-score normalization is:

[source,text]
----
z = (x - μ) / σ

Where:
- z = standardized value (Z-score)
- x = original value
- μ = mean of the dataset
- σ = standard deviation of the dataset
----


.Key Properties

After Z-score normalization:

* **Mean = 0**: The transformed data is centered at zero
* **Standard Deviation = 1**: The data has unit variance
* **Range**: Typically between -3 and +3, but theoretically unbounded
* **Distribution shape**: Preserved from the original data


.Step-by-Step Process

1. **Calculate the mean** of the dataset
2. **Calculate the standard deviation** of the dataset
3. **For each data point**, subtract the mean and divide by the standard deviation

.Visual Interpretation

* **Positive Z-scores**: Values above the mean
* **Negative Z-scores**: Values below the mean
* **Magnitude**: Indicates how many standard deviations away from the mean
image:z_score_housing.png[]


Advantages of Z-Score Normalization::
1. **Scale Independence**: Makes features with different units comparable
2. **Outlier Identification**: Values beyond ±3 standard deviations are easily identified as outliers
3. **Algorithm Performance**: Many ML algorithms perform better with standardized features
4. **Statistical Properties**: Preserves the shape of the original distribution
5. **Interpretability**: Z-scores have a clear statistical meaning

Disadvantages and Limitations::
1. **Unbounded Range**: Unlike min-max scaling, Z-scores are not bounded to a specific range
2. **Assumes Normal Distribution**: Works best when data is approximately normally distributed
3. **Sensitive to Outliers**: Extreme outliers can affect the mean and standard deviation
4. **Not Suitable for All Algorithms**: Some algorithms prefer bounded inputs (e.g., neural networks with sigmoid activation)



.When to Use Z-Score Normalization

Z-score normalization is particularly effective when:

* Working with algorithms that assume normally distributed data (e.g., Linear Regression, Logistic Regression)
* Features have different units or scales
* You need to identify outliers based on statistical thresholds
* Using distance-based algorithms (e.g., k-NN, k-Means, SVM)
* The data approximately follows a normal distribution



.Conclusion

Z-score normalization is a powerful preprocessing technique that standardizes features to have zero mean and unit variance. It's particularly valuable for:

* Making features with different scales comparable
* Improving the performance of many machine learning algorithms
* Identifying statistical outliers
* Ensuring numerical stability in optimization algorithms

While it has limitations (unbounded range, sensitivity to outliers), Z-score normalization remains one of the most widely used and effective scaling techniques in data science and machine learning.



==== Winsorizing
Winsorizing is a statistical technique used to handle outliers in datasets by replacing extreme values with less extreme values at specified percentiles. Named after biostatistician Charles P. Winsor, this method provides a robust alternative to **simply removing outliers**, preserving the sample size while reducing the impact of extreme observations.

The process of limiting extreme values in statistical data by replacing outliers with values at specified percentiles. Unlike trimming, winsorizing preserves all data points by transforming rather than removing them.


.Percentiles
Statistical measures that divide a dataset into 100 equal parts. For example:
- **5th percentile**: The value below which 5% of the data falls
- **95th percentile**: The value below which 95% of the data falls

image:winsorized.png[]

.Outliers
Data points that are significantly different from other observations in the dataset. These can be:
- **Legitimate extreme values**: Valid but unusual measurements
- **Measurement errors**: Incorrect data due to equipment malfunction or human error
- **Data entry errors**: Mistakes made during data collection or input


.Robust Statistics
Statistical methods that are not heavily influenced by outliers or deviations from model assumptions. Winsorizing creates more robust estimates by reducing outlier impact.



.Clipping Bounds
The threshold values (typically percentiles) used to determine which observations should be winsorized. Common choices include:
- **5th and 95th percentiles** (10% total winsorization)
- **1st and 99th percentiles** (2% total winsorization)
- **10th and 90th percentiles** (20% total winsorization)


.Step-by-Step Process
1. **Calculate Percentiles**: Determine the lower and upper bounds (e.g., 5th and 95th percentiles)
2. **Identify Outliers**: Find values below the lower bound or above the upper bound
3. **Replace Extreme Values**: 
   - Values below the lower bound → Set to lower bound value
   - Values above the upper bound → Set to upper bound value
4. **Preserve Sample Size**: All original data points remain, but extreme values are "pulled in"

image:winsorizing.png[]



.Properties
Advantages::
1. **Preserves Sample Size: ** Unlike trimming (which removes outliers), winsorizing keeps all data points, maintaining statistical power and avoiding information loss.
2. **Reduces Outlier Impact: ** Extreme values are capped at reasonable thresholds, preventing them from dominating statistical calculations while preserving their directional information.
3. **Data-Driven Approach: ** Uses percentiles from the actual data distribution rather than arbitrary fixed thresholds, making it adaptive to different datasets.
4. **Maintains Distribution : ** The overall shape and characteristics of the data distribution are largely preserved, unlike more aggressive transformations.
5. **Robust Statistical Estimates: ** Creates more stable estimates of central tendency and variability that are less sensitive to extreme observations.

Disadvantages and limitation::
1. **Information Loss: ** Extreme values lose their original magnitude, potentially removing important information about the true range of the phenomenon being measured.
2. **Threshold Selection Sensitivity: ** The choice of percentiles (5th/95th vs 1st/99th) can significantly impact results, and there's no universal "best" choice.
3. **May Mask Important Patterns: ** In some domains, extreme values represent critical events (e.g., fraud detection, rare diseases) that shouldn't be modified.
4. **Artificial Clustering: ** Creates artificial clusters of values at the winsorizing thresholds, which may affect certain types of analysis.


.When to use Winsorizing

- **Financial Data Analysis**: Stock returns, trading volumes, and financial ratios often contain extreme values that can skew analysis.

- **Survey Research**: Income, age, and other demographic variables frequently have outliers that don't represent the typical population.

- **Medical Research**: Biological measurements may contain extreme values due to measurement errors or rare conditions.

- **Quality Control**: Manufacturing data where extreme values might represent equipment malfunctions rather than true product variation.

.When to avoid

- **Extreme Values Are Meaningful**: In fraud detection or safety analysis, outliers often represent the most important cases.

- **Small Sample Sizes**: With limited data, every observation is valuable and winsorizing may remove too much information.

- **Regulatory Requirements**: Some industries require reporting actual values without modification.


.Conclusion
Winsorizing is a valuable technique for handling outliers that balances the need to reduce extreme value impact while preserving sample size and data structure. It's particularly useful in exploratory data analysis and when preparing data for algorithms sensitive to outliers.

The key to successful winsorizing lies in thoughtful threshold selection, careful validation of results, and clear documentation of the process. When applied appropriately, winsorizing can significantly improve the robustness of statistical analyses while maintaining the integrity of the underlying data patterns.



==== Boxcox
The Box-Cox transformation is a powerful statistical technique that transforms non-normal data into a normal distribution. Named after statisticians George Box and David Cox who developed it in 1964, this transformation is particularly useful when your data violates the normality assumption required by many statistical methods.

[source,math]
----
y(λ) = { (y^λ - 1) / λ    if λ ≠ 0
       { log(y)           if λ = 0
----

Where:
- `y` is the original data (must be positive).
- `λ` (lambda) is the transformation parameter.
- `y(λ)` is the transformed data.

.Common transformation based on λ:
- λ = -1: Reciprocal (1/y)
- λ = 0: Natural log
- λ = 0.5: Square root
- λ = 1: No transformation (y-1)
- λ = 2: Square

The goal is to find the optimal value of `λ` that makes the data as close to normally distributed as possible.

image:boxcox.png[]

.Why Use Box-Cox Transformation?

1. **Stabilize Variance**:
   The transformation reduces heteroscedasticity, making the variance more uniform across the range of values.

2. **Normalize Data**:
   Many statistical methods assume normally distributed data. Box-Cox helps meet this assumption.

3. **Improve Model Performance**:
   Machine learning models often perform better with normalized features.

4. **Handle Skewness**:
   The transformation reduces skewness, making the data more symmetric.



