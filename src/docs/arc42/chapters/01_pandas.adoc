:jbake-title: Pandas
:jbake-type: page_toc
:jbake-status: published
:jbake-menu: arc42
:jbake-order: 1
:filename: /chapters/01_pandas.adoc
ifndef::imagesdir[:imagesdir: ../../images]

:toc:



[[section-building-block-view]]


== Pandas

=== Data Access & Select

This document outlines common pandas functions for selecting and inspecting data, mirroring the style used in *03_scaling_numerical.adoc*. Use it as a quick reference for exploring and manipulating DataFrames.





==== df.head()
.Shows the first few rows of a DataFrame.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35]
})

print(df.head())  # By default, shows first 5 rows
----
*When to Use:*  
- Quickly inspect the top of a dataset to verify columns, data types, and sample records.

*When Not to Use:*  
- Large-scale data exploration beyond the first few rows.





==== df.tail()
.Shows the last few rows of a DataFrame.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Ethan'],
    'Score': [88, 92, 79, 85, 90]
})

print(df.tail(2))  # Shows the last 2 rows
----
*When to Use:*  
- Check recent rows, e.g., last few records in a time series.

*When Not to Use:*  
- Full data inspection or large-scale analysis.





==== df.describe()
.Provides summary statistics for numeric columns.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'col1': [10, 20, 30],
    'col2': [5, 10, 15]
})

print(df.describe())
----
*When to Use:*  
- Get quick insight into count, mean, standard deviation, etc.

*When Not to Use:*  
- Investigating non-numeric columns in detail (it won’t summarize strings).


==== df.sort_index()
.Sorts a DataFrame by its index.
[source,python]
----
import pandas as pd
df = pd.DataFrame({
    'Name': ['Charlie', 'Bob', 'Alice'],
    'Age': [35, 30, 25]
}, index=[2, 1, 0])

sorted_df = df.sort_index()
print(sorted_df)
----
*When to Use:*
- When you need to order data based on the DataFrame's index.
- Useful for time series data or when the index is meaningful.
*When Not to Use:*
- When you need to sort by a specific column instead of the index; use `df.sort_values()` instead.




==== df.sort_values('col')
.Sorts a DataFrame by a specified column.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'Name': ['Charlie', 'Bob', 'Alice'],
    'Age': [35, 30, 25]
})

sorted_df = df.sort_values('Age')
print(sorted_df)
----
*When to Use:*  
- Order data for better readability or to prepare for merges/joins.

*When Not to Use:*  
- Data that is too large for in-memory sorting.




==== .loc[row, col]
.Label-based data selection.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'City': ['Paris', 'London', 'Berlin', 'London'],
    'Population': [2148271, 8982000, 3645000, 1234567]
})
# loc is label-based
pop_london = df.loc[df['City'] == 'London', 'Population']
print(pop_london)

# or you can use a mask with .loc
mask = df['City'].eq('London')
pop_london_mask = df.loc[mask, 'Population']
print(pop_london_mask)

----

Output:
[source,python]
----
1    8982000
3    1234567
Name: Population, dtype: int64
----

.How it works:
Loc parses its two arguments as follows:
* .loc sees a boolean array on the first axis, keeps all rows whose mask element is True, and then returns the Population column of those rows.
Example:
* 0 False
* 1 True
* 2 False


*When to Use:*  
- Selecting rows and columns by labels or boolean conditions.

*When Not to Use:*  
- Pure index-based selection; use .iloc instead.

==== .iloc[row, col]
.Index-based (positional) data selection.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'City': ['Paris', 'London', 'Berlin', 'London'],
    'Population': [2148271, 8982000, 3645000, 1234567]
})

#iloc is index-based
pop_london_iloc = df.iloc[2, :]  # 0 is index of city, 1 is index of population, use : to select all columns
print(pop_london_iloc)
----
.Output:
[source,python]
----
City           Berlin
Population    3645000
Name: 2, dtype: object
----
.Or if you want to make it the same as .loc where it returns all the rows that passed the condition:
[source,python]
----
# convert the column label to its positional index
pop_idx = df.columns.get_loc('Population')
# Alternatively, to use .iloc, convert the mask to integer positions:
row_indices = mask[mask].index
# same rows/column as your .loc version
pop_london = df.iloc[row_indices, pop_idx]
print(pop_london)
----
.Output:
[source,python]
----
1    8982000
3    1234567
Name: Population, dtype: int64
----



*When to Use:*  
- Access by integer positions (like array indexing).

*When Not to Use:*  
- Selecting by label or condition; use .loc instead.

==== .at[row, col]
.Fast label-based single value access.
[source,python]
----
selected_idx = 1
val = df.at[selected_idx, 'City']  # Accessing the Population of London using label
val2 = df.at[selected_idx, 'Population']  # Accessing the Population of London using label
print(val, val2)
----
.Output:
[source,python]
----
London 8982000
----
*When to Use:*  
- Optimized for retrieving a single value at known row and column labels.

*When Not to Use:*  
- Selecting multiple rows or columns at once.


==== .iat[row, col]

.Fast index-based single value access.
[source,python]
----
london_pop = df.iat[1, 1]  # Accessing the Population of London using index
print(london_pop)  
----

.Output:
[source,python]
----
8982000
----
*When to Use:*  
- Optimized for retrieving a single value at known row and column positions.

*When Not to Use:*  
- Selecting by label or for retrieving multiple values.




==== Boolean Masks
.Boolean masks are a powerful feature in pandas that allow you to filter DataFrames based on specific conditions.
[source,python]
----
mask = df['Population'] >= 3000000
filtered_df = df[mask]
print(filtered_df.iloc[::, ::]) # Display all columns, or just use head() to show the first few rows
----
.Output:
[source,python]
----
       City  Population
1   London      8982000
2   Berlin      3645000
----
*When to Use:*  
- When you need to filter data based on specific conditions.

*When Not to Use:*  
- For simple row/column selection; use .loc or .iloc instead.

.One-liner boolean mask:
[source,python]
----
filtered_df = df[df['Population'].ge(8_982_000)]   # .ge == “>=”
----


=== Missing Values
==== df.dropna()
.Removes rows (or columns) that contain missing values.
[source,python]
----
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'Name' : ['Alice', 'Bob',   np.nan, 'Diana', np.nan],
    'Score': [  88 ,   np.nan,   79  ,   90  , np.nan ], 
    'City' : [np.nan, np.nan, np.nan, np.nan, np.nan],
})

print("Original DataFrame:", df.iloc[::,::], sep='\n')

# 1️⃣  Drop any row that has at least one NaN
clean_rows_any = df.dropna()    #default is axis=rows, how ='any'
print('clean rows: ',clean_rows_any, sep='\n')  
#this will return empty dataframe because all rows have at least one NaN

clean_rows_all = df.dropna(how='all')   # this will drop rows only if all values are NaN
print('clean rows all: ',clean_rows_all, sep='\n')


# 2️⃣  Drop columns that are all NaN
clean_cols = df.dropna(axis='columns', how='all')
print('clean_col', clean_cols, sep='\n')
----
.Output:
[source,python]
----
Original DataFrame:
    Name  Score  City
0  Alice   88.0   NaN
1    Bob    NaN   NaN
2    NaN   79.0   NaN
3  Diana   90.0   NaN
4    NaN    NaN   NaN
clean rows: 
Empty DataFrame
Columns: [Name, Score, City]
Index: []
clean rows all: 
    Name  Score  City
0  Alice   88.0   NaN
1    Bob    NaN   NaN
2    NaN   79.0   NaN
3  Diana   90.0   NaN
clean_col
    Name  Score
0  Alice   88.0
1    Bob    NaN
2    NaN   79.0
3  Diana   90.0
4    NaN    NaN
----
*When to Use:*  
- You truly want to discard incomplete records (e.g., training a model that cannot handle NaNs).  
- Columns are completely empty and add no information.

*When Not to Use:*  
- The loss of data would harm analysis or create bias.  
- You only need to ignore NaNs temporarily—consider `.fillna()` or model-side handling instead.



==== df.fillna()
.Replaces missing values with a specified constant or computed statistic.
[source,python]
----
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'Product': ['A', 'B', 'C', 'D'],
    'Price'  : [10.5, np.nan, 12.0, np.nan]
})

# 1️⃣  Fill with a scalar
df_fixed = df.fillna(0)
print(df_fixed)

# 2️⃣  Fill with a column mean (common in feature engineering)
mean_price = df['Price'].mean()
df_mean = df.assign(Price=df['Price'].fillna(mean_price))
print(df_mean)
----
.Output:
[source,python]
----
  Product  Price
0       A   10.5
1       B    0.0
2       C   12.0
3       D    0.0

  Product  Price
0       A  10.50
1       B  11.25
2       C  12.00
3       D  11.25
----
*When to Use:*  
- Keeping the row is more valuable than the exact value (e.g., imputing with mean/median).  
- Preparing data for ML algorithms that require complete numeric input.

*When Not to Use:*  
- The placeholder would distort downstream statistics (e.g., filling with 0 when 0 is a valid, meaningful value).  
- Missingness itself carries information—you might add a “_missing” indicator instead.



==== df.isna()
.Returns a Boolean mask indicating missing values (NaN or None).
[source,python]
----
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'City'      : ['Paris', 'London', None,   'Berlin'],
    'Population': [2_148_000, np.nan, 3_645_000, 3_748_000]
})

mask = df.isna()
print(mask)

# Example: count missing cells per column
missing_per_col = mask.sum()
print(missing_per_col)
----
.Output:
[source,python]
----
    City  Population
0  False       False
1  False        True
2   True       False
3  False       False

City          1
Population    1
dtype: int64
----
*When to Use:*  
- Building custom boolean masks for advanced filtering (e.g., `df[df['Population'].isna()]`).  
- Quick diagnostics or QA checks (`df.isna().sum()` for a nulls overview).

*When Not to Use:*  
- Situations requiring the *inverse*—in that case use `.notna()` for readability.  


=== Custom Transformations
==== .agg()
.Aggregation on a DataFrame or a GroupBy object.
[source,python]
----
import pandas as pd
import numpy as np

df = pd.DataFrame({
    "Team": ["A", "A", "B", "B"],
    "Points": [10, 12, 7, 9],
    "Assists": [5, 7, 3, 4]
})

# 1️⃣  Single aggregate on the whole DataFrame
totals = df.agg("sum")
print(totals)

# 2️⃣  Multiple aggregates after grouping
print(df.head())
team_stats = (
    df.groupby("Team")
        .agg(
            pts_mean=("Points", "mean"),     # named aggregation
            ast_sum =("Assists", "sum")
        )
)
print(team_stats)
----
.Output:
[source,python]
----
Team       AABB
Points       38
Assists      19
dtype: object
  Team  Points  Assists
0    A      10        5
1    A      12        7
2    B       7        3
3    B       9        4
      pts_mean  ast_sum
Team                   
A         11.0       12
B          8.0        7
----
*When to Use:*  
- You need one-row-per-group summaries such as mean, sum, min/max, count, etc.  
- You want several different functions at once (named aggregation syntax is concise). 

*When Not to Use:*  
- You must return an object the same shape as the original (use `.transform()` instead).  
- The function you apply is not reduction-like (e.g. standardising each value).


==== .transform()
.Element-wise transform that returns an object of the same size.
[source,python]
----
df['Quadrat_point'] = df['Points'].transform(lambda x: x**2) # applies the function lamba x: x**2 to each element in the "Points" column
print(df.head())  
----
.Output:
[source,python]
----
Team  Points  Assists  Points_z  Quadrat_point
0    A      10        5   0.27735            100
1    A      12        7   1.38675            144
2    B       7        3  -1.38675             49
3    B       9        4  -0.27735             81
----
*When to Use:*  
- You want to broadcast a group-level calculation back to every row (e.g. z-scores, filling NA with group median). [[1]]  
- Feature engineering steps that must preserve row order and length (ML pipelines).

*When Not to Use:*  
- You only need one row per group (prefer `.agg()` for efficiency).  
- The transformation cannot be vectorised and runs slowly row-by-row—then consider `.apply()` as a last resort.

==== Quick rule of thumb
• `.agg()` = “reduce” →  shrinks each group to one row (or one scalar).  
• `.transform()` = “broadcast” →  keeps original shape; every input row re-appears in the output.  
If your custom function cannot satisfy either contract, fall back to `.apply()` (it has no shape promise but is slower). [[1]]




=== df.groupby()
.Group data by one or more keys, then apply aggregations or transformations.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    "Team"  : ["A", "A", "B", "B"],
    "Points": [10, 12, 7,  9 ],
    "Assists": [5, 7, 3, 4]
})

# 1️⃣  Global mean over all rows --------------
x_means = df["Points"].mean()
print("Global mean:", x_means)

# 2️⃣  Mean *per team* ------------------------
x_means_grouped = df.groupby("Team")["Points"].mean()
print("\nMean per team:\n", x_means_grouped)
----
.Output:
[source,python]
----
Global mean: 9.5

Mean per team:
Team
A    11.0
B     8.0
Name: Points, dtype: float64
----

.How it works:
1. `df["Points"].mean()` looks at the entire column—one number out.
2. `df.groupby("Team")` splits the frame into two sub-tables (Team A, Team B).  
   Calling `.mean()` on the `Points` column is then applied to each piece **before** the results are combined.

*When to Use:*  
- Any time you need per-group statistics: sums, means, counts, custom lambdas, etc.  
- Preparing features such as “player’s points minus team average.”  
- Rolling up data for reports or dashboards.

*When Not to Use:*  
- When your operation does **not** depend on grouping (the plain column method is faster).  
- When you only need element-wise transforms that keep the original shape—use `.transform()` instead.  

.Tips:
• Add `as_index=False` if you want the group labels to become a normal column rather than the index.  
• Chain multiple aggregates with `.agg()` for concise, named output:  
  `df.groupby("Team").agg(mean_pts=("Points","mean"), max_ast=("Assists","max"))`  
• Turn off automatic sorting with `sort=False` to keep original row order.  



=== Concatenating DataFrames
==== pd.concat()
.Concatenate (stack) Series/DataFrames vertically or horizontally.
[source,python]
----
import pandas as pd

df1 = pd.DataFrame({"id": [1, 2], "A": ["A0", "A1"]})
df2 = pd.DataFrame({"id": [4, 5], "A": ["A2", "A3"]})

# 1️⃣  Vertikales Stapeln  (axis=0 ist Standard)
vstack = pd.concat([df1, df2], axis=0, ignore_index=True)
print(vstack)

# 2️⃣  Horizontales Stapeln  (axis=1)
hstack = pd.concat([df1.set_index("id"), df2.set_index("id")], axis=1,
                   join="outer")
print(hstack)
----
.Output:
[source,python]
----
   id   A
0   1  A0
1   2  A1
2   4  A2
3   5  A3
      A    A
id          
1    A0  NaN
2    A1  NaN
4   NaN   A2
5   NaN   A3
----

*Performance-Warnung – nicht Zeile-für-Zeile konkatenieren*  
`pd.concat()` muss bei jedem Aufruf kopieren. In einer Schleife entsteht so
viel unnötige Arbeit:

[source,python]
----
# ❌ langsam
out = pd.DataFrame()
for row in rows:
    out = pd.concat([out, row.to_frame().T])
----

Besser: erst sammeln, dann einmal konkatenieren:

[source,python]
----
frames = [row.to_frame().T for row in rows]   # billig
out = pd.concat(frames, ignore_index=True)    # nur eine Kopie
----

*When to Use:*  
- Stapeln von DataFrames mit gleichen Spalten (axis=0) oder passenden
  Indizes (axis=1).  
- Mehrere neue Zeilen in einem Rutsch anhängen.

*When Not to Use:*  
- Schlüsselbasierte Joins → `pd.merge()`.  
- Live-Streaming einzelner Zeilen → lieber Liste puffern oder direkt DB.




==== pd.merge()
.SQL-ähnliche Joins (inner, left, right, outer, cross).
[source,python]
----
import pandas as pd

left  = pd.DataFrame({"id": [1, 2, 3], "points": [10, 20, 30]})
right = pd.DataFrame({"id": [2, 3, 4], "grade":  ["B", "A", "C"]})

# 1️⃣  Inner Join – nur übereinstimmende ids
inner = pd.merge(left, right, on="id")
print(inner)

# 2️⃣  Left Join – alle Zeilen aus left behalten
left_join = pd.merge(left, right, on="id", how="left")
print(left_join)

# 3️⃣  Kreuzprodukt (cross)
cross = pd.merge(left, right, how="cross")
print(cross.head())   # 9 Zeilen = 3×3
----
.Output (gekürzt):
[source,python]
----
    id  points grade
0   2      20     B
1   3      30     A

    id  points grade
0   1      10   NaN
1   2      20     B
2   3      30     A

    id_x  points  id_y grade
0     1      10     2     B
1     1      10     3     A
2     1      10     4     C
3     2      20     2     B
4     2      20     3     A
----


*Kurzübersicht how=*  
inner – Schnittmenge (Standard)  
left  – wie SQL LEFT JOIN  
right – RIGHT JOIN  
outer – vollständige Vereinigung  
cross – kartesisches Produkt

*When to Use:*  
- Tabellen über Schlüsselspalten kombinieren.  
- Dimensionstabellen anhängen, Many-to-Many-Joins.

*When Not to Use:*  
- Einfaches Reihen/Spalten-Anfügen → `pd.concat()`.  
- Joins nur über Index → `DataFrame.join()` ist oft klarer.

.Tipps  
* `indicator=True` fügt Spalte `_merge` mit Herkunftsinfo hinzu.  
* Doppelte Spaltennamen per `suffixes=('_l', '_r')` auflösen.  
* Für zeitnahe “nearest” Joins gibt es `pd.merge_asof()`.

