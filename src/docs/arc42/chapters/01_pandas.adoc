:jbake-title: Pandas
:jbake-type: page_toc
:jbake-status: published
:jbake-menu: arc42
:jbake-order: 1
:filename: /chapters/01_pandas.adoc
ifndef::imagesdir[:imagesdir: ../../images]

:toc:



[[section-building-block-view]]


== Pandas

=== Data Access & Select

This document outlines common pandas functions for selecting and inspecting data, mirroring the style used in *03_scaling_numerical.adoc*. Use it as a quick reference for exploring and manipulating DataFrames.





==== df.head()
.Shows the first few rows of a DataFrame.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35]
})

print(df.head())  # By default, shows first 5 rows
----
*When to Use:*  
- Quickly inspect the top of a dataset to verify columns, data types, and sample records.

*When Not to Use:*  
- Large-scale data exploration beyond the first few rows.





==== df.tail()
.Shows the last few rows of a DataFrame.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Ethan'],
    'Score': [88, 92, 79, 85, 90]
})

print(df.tail(2))  # Shows the last 2 rows
----
*When to Use:*  
- Check recent rows, e.g., last few records in a time series.

*When Not to Use:*  
- Full data inspection or large-scale analysis.





==== df.describe()
.Provides summary statistics for numeric columns.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'col1': [10, 20, 30],
    'col2': [5, 10, 15]
})

print(df.describe())
----
*When to Use:*  
- Get quick insight into count, mean, standard deviation, etc.

*When Not to Use:*  
- Investigating non-numeric columns in detail (it won’t summarize strings).


==== df.sort_index()
.Sorts a DataFrame by its index.
[source,python]
----
import pandas as pd
df = pd.DataFrame({
    'Name': ['Charlie', 'Bob', 'Alice'],
    'Age': [35, 30, 25]
}, index=[2, 1, 0])

sorted_df = df.sort_index()
print(sorted_df)
----
*When to Use:*
- When you need to order data based on the DataFrame's index.
- Useful for time series data or when the index is meaningful.
*When Not to Use:*
- When you need to sort by a specific column instead of the index; use `df.sort_values()` instead.




==== df.sort_values('col')
.Sorts a DataFrame by a specified column.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'Name': ['Charlie', 'Bob', 'Alice'],
    'Age': [35, 30, 25]
})

sorted_df = df.sort_values('Age')
print(sorted_df)
----
*When to Use:*  
- Order data for better readability or to prepare for merges/joins.

*When Not to Use:*  
- Data that is too large for in-memory sorting.




==== .loc[row, col]
.Label-based data selection.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'City': ['Paris', 'London', 'Berlin', 'London'],
    'Population': [2148271, 8982000, 3645000, 1234567]
})
# loc is label-based
pop_london = df.loc[df['City'] == 'London', 'Population']
print(pop_london)

# or you can use a mask with .loc
mask = df['City'].eq('London')
pop_london_mask = df.loc[mask, 'Population']
print(pop_london_mask)

----

Output:
[source,python]
----
1    8982000
3    1234567
Name: Population, dtype: int64
----

.How it works:
Loc parses its two arguments as follows:
* .loc sees a boolean array on the first axis, keeps all rows whose mask element is True, and then returns the Population column of those rows.
Example:
* 0 False
* 1 True
* 2 False


*When to Use:*  
- Selecting rows and columns by labels or boolean conditions.

*When Not to Use:*  
- Pure index-based selection; use .iloc instead.

==== .iloc[row, col]
.Index-based (positional) data selection.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    'City': ['Paris', 'London', 'Berlin', 'London'],
    'Population': [2148271, 8982000, 3645000, 1234567]
})

#iloc is index-based
pop_london_iloc = df.iloc[2, :]  # 0 is index of city, 1 is index of population, use : to select all columns
print(pop_london_iloc)
----
.Output:
[source,python]
----
City           Berlin
Population    3645000
Name: 2, dtype: object
----
.Or if you want to make it the same as .loc where it returns all the rows that passed the condition:
[source,python]
----
# convert the column label to its positional index
pop_idx = df.columns.get_loc('Population')
# Alternatively, to use .iloc, convert the mask to integer positions:
row_indices = mask[mask].index
# same rows/column as your .loc version
pop_london = df.iloc[row_indices, pop_idx]
print(pop_london)
----
.Output:
[source,python]
----
1    8982000
3    1234567
Name: Population, dtype: int64
----



*When to Use:*  
- Access by integer positions (like array indexing).

*When Not to Use:*  
- Selecting by label or condition; use .loc instead.

==== .at[row, col]
.Fast label-based single value access.
[source,python]
----
selected_idx = 1
val = df.at[selected_idx, 'City']  # Accessing the Population of London using label
val2 = df.at[selected_idx, 'Population']  # Accessing the Population of London using label
print(val, val2)
----
.Output:
[source,python]
----
London 8982000
----
*When to Use:*  
- Optimized for retrieving a single value at known row and column labels.

*When Not to Use:*  
- Selecting multiple rows or columns at once.


==== .iat[row, col]

.Fast index-based single value access.
[source,python]
----
london_pop = df.iat[1, 1]  # Accessing the Population of London using index
print(london_pop)  
----

.Output:
[source,python]
----
8982000
----
*When to Use:*  
- Optimized for retrieving a single value at known row and column positions.

*When Not to Use:*  
- Selecting by label or for retrieving multiple values.




==== Boolean Masks
.Boolean masks are a powerful feature in pandas that allow you to filter DataFrames based on specific conditions.
[source,python]
----
mask = df['Population'] >= 3000000
filtered_df = df[mask]
print(filtered_df.iloc[::, ::]) # Display all columns, or just use head() to show the first few rows
----
.Output:
[source,python]
----
       City  Population
1   London      8982000
2   Berlin      3645000
----
*When to Use:*  
- When you need to filter data based on specific conditions.

*When Not to Use:*  
- For simple row/column selection; use .loc or .iloc instead.

.One-liner boolean mask:
[source,python]
----
filtered_df = df[df['Population'].ge(8_982_000)]   # .ge == “>=”
----


=== Missing Values
==== df.dropna()
.Removes rows (or columns) that contain missing values.
[source,python]
----
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'Name' : ['Alice', 'Bob',   np.nan, 'Diana', np.nan],
    'Score': [  88 ,   np.nan,   79  ,   90  , np.nan ], 
    'City' : [np.nan, np.nan, np.nan, np.nan, np.nan],
})

print("Original DataFrame:", df.iloc[::,::], sep='\n')

# 1️⃣  Drop any row that has at least one NaN
clean_rows_any = df.dropna()    #default is axis=rows, how ='any'
print('clean rows: ',clean_rows_any, sep='\n')  
#this will return empty dataframe because all rows have at least one NaN

clean_rows_all = df.dropna(how='all')   # this will drop rows only if all values are NaN
print('clean rows all: ',clean_rows_all, sep='\n')


# 2️⃣  Drop columns that are all NaN
clean_cols = df.dropna(axis='columns', how='all')
print('clean_col', clean_cols, sep='\n')
----
.Output:
[source,python]
----
Original DataFrame:
    Name  Score  City
0  Alice   88.0   NaN
1    Bob    NaN   NaN
2    NaN   79.0   NaN
3  Diana   90.0   NaN
4    NaN    NaN   NaN
clean rows: 
Empty DataFrame
Columns: [Name, Score, City]
Index: []
clean rows all: 
    Name  Score  City
0  Alice   88.0   NaN
1    Bob    NaN   NaN
2    NaN   79.0   NaN
3  Diana   90.0   NaN
clean_col
    Name  Score
0  Alice   88.0
1    Bob    NaN
2    NaN   79.0
3  Diana   90.0
4    NaN    NaN
----
*When to Use:*  
- You truly want to discard incomplete records (e.g., training a model that cannot handle NaNs).  
- Columns are completely empty and add no information.

*When Not to Use:*  
- The loss of data would harm analysis or create bias.  
- You only need to ignore NaNs temporarily—consider `.fillna()` or model-side handling instead.



==== df.fillna()
.Replaces missing values with a specified constant or computed statistic.
[source,python]
----
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'Product': ['A', 'B', 'C', 'D'],
    'Price'  : [10.5, np.nan, 12.0, np.nan]
})

# 1️⃣  Fill with a scalar
df_fixed = df.fillna(0)
print(df_fixed)

# 2️⃣  Fill with a column mean (common in feature engineering)
mean_price = df['Price'].mean()
df_mean = df.assign(Price=df['Price'].fillna(mean_price))
print(df_mean)
----
.Output:
[source,python]
----
  Product  Price
0       A   10.5
1       B    0.0
2       C   12.0
3       D    0.0

  Product  Price
0       A  10.50
1       B  11.25
2       C  12.00
3       D  11.25
----
*When to Use:*  
- Keeping the row is more valuable than the exact value (e.g., imputing with mean/median).  
- Preparing data for ML algorithms that require complete numeric input.

*When Not to Use:*  
- The placeholder would distort downstream statistics (e.g., filling with 0 when 0 is a valid, meaningful value).  
- Missingness itself carries information—you might add a “_missing” indicator instead.



==== df.isna()
.Returns a Boolean mask indicating missing values (NaN or None).
[source,python]
----
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'City'      : ['Paris', 'London', None,   'Berlin'],
    'Population': [2_148_000, np.nan, 3_645_000, 3_748_000]
})

mask = df.isna()
print(mask)

# Example: count missing cells per column
missing_per_col = mask.sum()
print(missing_per_col)
----
.Output:
[source,python]
----
    City  Population
0  False       False
1  False        True
2   True       False
3  False       False

City          1
Population    1
dtype: int64
----
*When to Use:*  
- Building custom boolean masks for advanced filtering (e.g., `df[df['Population'].isna()]`).  
- Quick diagnostics or QA checks (`df.isna().sum()` for a nulls overview).

*When Not to Use:*  
- Situations requiring the *inverse*—in that case use `.notna()` for readability.  


=== Custom Transformations
==== .agg()
.Aggregation on a DataFrame or a GroupBy object.
[source,python]
----
import pandas as pd
import numpy as np

df = pd.DataFrame({
    "Team": ["A", "A", "B", "B"],
    "Points": [10, 12, 7, 9],
    "Assists": [5, 7, 3, 4]
})

# 1️⃣  Single aggregate on the whole DataFrame
totals = df.agg("sum")
print(totals)

# 2️⃣  Multiple aggregates after grouping
print(df.head())
team_stats = (
    df.groupby("Team")
        .agg(
            pts_mean=("Points", "mean"),     # named aggregation
            ast_sum =("Assists", "sum")
        )
)
print(team_stats)
----
.Output:
[source,python]
----
Team       AABB
Points       38
Assists      19
dtype: object
  Team  Points  Assists
0    A      10        5
1    A      12        7
2    B       7        3
3    B       9        4
      pts_mean  ast_sum
Team                   
A         11.0       12
B          8.0        7
----
*When to Use:*  
- You need one-row-per-group summaries such as mean, sum, min/max, count, etc.  
- You want several different functions at once (named aggregation syntax is concise). 

*When Not to Use:*  
- You must return an object the same shape as the original (use `.transform()` instead).  
- The function you apply is not reduction-like (e.g. standardising each value).


==== .transform()
.Element-wise transform that returns an object of the same size.
[source,python]
----
df['Quadrat_point'] = df['Points'].transform(lambda x: x**2) # applies the function lamba x: x**2 to each element in the "Points" column
print(df.head())  
----
.Output:
[source,python]
----
Team  Points  Assists  Points_z  Quadrat_point
0    A      10        5   0.27735            100
1    A      12        7   1.38675            144
2    B       7        3  -1.38675             49
3    B       9        4  -0.27735             81
----
*When to Use:*  
- You want to broadcast a group-level calculation back to every row (e.g. z-scores, filling NA with group median). [[1]]  
- Feature engineering steps that must preserve row order and length (ML pipelines).

*When Not to Use:*  
- You only need one row per group (prefer `.agg()` for efficiency).  
- The transformation cannot be vectorised and runs slowly row-by-row—then consider `.apply()` as a last resort.

==== Quick rule of thumb
• `.agg()` = “reduce” →  shrinks each group to one row (or one scalar).  
• `.transform()` = “broadcast” →  keeps original shape; every input row re-appears in the output.  
If your custom function cannot satisfy either contract, fall back to `.apply()` (it has no shape promise but is slower). [[1]]




=== df.groupby()
.Group data by one or more keys, then apply aggregations or transformations.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    "Team"  : ["A", "A", "B", "B"],
    "Points": [10, 12, 7,  9 ],
    "Assists": [5, 7, 3, 4]
})

# 1️⃣  Global mean over all rows --------------
x_means = df["Points"].mean()
print("Global mean:", x_means)

# 2️⃣  Mean *per team* ------------------------
x_means_grouped = df.groupby("Team")["Points"].mean()
print("\nMean per team:\n", x_means_grouped)
----
.Output:
[source,python]
----
Global mean: 9.5

Mean per team:
Team
A    11.0
B     8.0
Name: Points, dtype: float64
----

.How it works:
1. `df["Points"].mean()` looks at the entire column—one number out.
2. `df.groupby("Team")` splits the frame into two sub-tables (Team A, Team B).  
   Calling `.mean()` on the `Points` column is then applied to each piece **before** the results are combined.

*When to Use:*  
- Any time you need per-group statistics: sums, means, counts, custom lambdas, etc.  
- Preparing features such as “player’s points minus team average.”  
- Rolling up data for reports or dashboards.

*When Not to Use:*  
- When your operation does **not** depend on grouping (the plain column method is faster).  
- When you only need element-wise transforms that keep the original shape—use `.transform()` instead.  

.Tips:
• Add `as_index=False` if you want the group labels to become a normal column rather than the index.  
• Chain multiple aggregates with `.agg()` for concise, named output:  
  `df.groupby("Team").agg(mean_pts=("Points","mean"), max_ast=("Assists","max"))`  
• Turn off automatic sorting with `sort=False` to keep original row order.  



=== Concatenating DataFrames
==== pd.concat()
.Concatenate (stack) Series/DataFrames vertically or horizontally.
[source,python]
----
import pandas as pd

df1 = pd.DataFrame({"id": [1, 2], "A": ["A0", "A1"]})
df2 = pd.DataFrame({"id": [4, 5], "A": ["A2", "A3"]})

# 1️⃣  Vertikales Stapeln  (axis=0 ist Standard)
vstack = pd.concat([df1, df2], axis=0, ignore_index=True)
print(vstack)

# 2️⃣  Horizontales Stapeln  (axis=1)
hstack = pd.concat([df1.set_index("id"), df2.set_index("id")], axis=1,
                   join="outer")
print(hstack)
----
.Output:
[source,python]
----
   id   A
0   1  A0
1   2  A1
2   4  A2
3   5  A3
      A    A
id          
1    A0  NaN
2    A1  NaN
4   NaN   A2
5   NaN   A3
----

*Performance-Warnung – nicht Zeile-für-Zeile konkatenieren*  
`pd.concat()` muss bei jedem Aufruf kopieren. In einer Schleife entsteht so
viel unnötige Arbeit:

[source,python]
----
# ❌ langsam
out = pd.DataFrame()
for row in rows:
    out = pd.concat([out, row.to_frame().T])
----

Besser: erst sammeln, dann einmal konkatenieren:

[source,python]
----
frames = [row.to_frame().T for row in rows]   # billig
out = pd.concat(frames, ignore_index=True)    # nur eine Kopie
----

*When to Use:*  
- Stapeln von DataFrames mit gleichen Spalten (axis=0) oder passenden
  Indizes (axis=1).  
- Mehrere neue Zeilen in einem Rutsch anhängen.

*When Not to Use:*  
- Schlüsselbasierte Joins → `pd.merge()`.  
- Live-Streaming einzelner Zeilen → lieber Liste puffern oder direkt DB.




==== pd.merge()
.SQL-ähnliche Joins (inner, left, right, outer, cross).
[source,python]
----
import pandas as pd

left  = pd.DataFrame({"id": [1, 2, 3], "points": [10, 20, 30]})
right = pd.DataFrame({"id": [2, 3, 4], "grade":  ["B", "A", "C"]})

# 1️⃣  Inner Join – nur übereinstimmende ids
inner = pd.merge(left, right, on="id")
print(inner)

# 2️⃣  Left Join – alle Zeilen aus left behalten
left_join = pd.merge(left, right, on="id", how="left")
print(left_join)

# 3️⃣  Kreuzprodukt (cross)
cross = pd.merge(left, right, how="cross")
print(cross.head())   # 9 Zeilen = 3×3
----
.Output (gekürzt):
[source,python]
----
    id  points grade
0   2      20     B
1   3      30     A

    id  points grade
0   1      10   NaN
1   2      20     B
2   3      30     A

    id_x  points  id_y grade
0     1      10     2     B
1     1      10     3     A
2     1      10     4     C
3     2      20     2     B
4     2      20     3     A
----


*Kurzübersicht how=*  
inner – Schnittmenge (Standard)  
left  – wie SQL LEFT JOIN  
right – RIGHT JOIN  
outer – vollständige Vereinigung  
cross – kartesisches Produkt

*When to Use:*  
- Tabellen über Schlüsselspalten kombinieren.  
- Dimensionstabellen anhängen, Many-to-Many-Joins.

*When Not to Use:*  
- Einfaches Reihen/Spalten-Anfügen → `pd.concat()`.  
- Joins nur über Index → `DataFrame.join()` ist oft klarer.

.Tipps  
* `indicator=True` fügt Spalte `_merge` mit Herkunftsinfo hinzu.  
* Doppelte Spaltennamen per `suffixes=('_l', '_r')` auflösen.  
* Für zeitnahe “nearest” Joins gibt es `pd.merge_asof()`.



==== melt()
.Unpivot → macht aus einer breiten Tabelle eine lange, schmale.
[source,python]
----
import pandas as pd

df = pd.DataFrame({
    "id": [1, 2],
    "Jan": [11, 12],
    "Feb": [21, 22],
    "Mar": [31, 32]
})
print(df)
# Klassische Anwendung: Monate stehen als Spalten, gehören aber in Zeilen
long = pd.melt(
    df,
    id_vars="id",                 # bleibt stehen
    value_vars=df.columns[1:],  # columns which should be melted
    # or value_vars=["Jan", "Feb", "Mar"],  if you want to specify manually
    var_name="Monat",             # neue Spalte mit den alten Spalten-Labels
    value_name="Umsatz"           # neue Spalte mit den Werten
)
print(long)
----
.Output:
[source,python]
----
   id  Jan  Feb  Mar
0   1   11   21   31
1   2   12   22   32
   id Monat  Umsatz
0   1   Jan      11
1   2   Jan      12
2   1   Feb      21
3   2   Feb      22
4   1   Mar      31
5   2   Mar      32
----
*When to Use:*  
- Spalten sind eigentlich Ausprägungen einer Variablen (z. B. Monate, Messpunkte).  
- Vorbereitung für `groupby`, Zeitreihen-Tools oder ggplot/Seaborn (tall/long-Format).

*When Not to Use:*  
- Wenn Spalten echte verschiedene Variablen bleiben sollen.  
- Datensatz ist bereits im Long-Format.




==== df.stack()
.Pivots the *column axis* into the *row index*; useful even with a single-level column index.
[source,python]
----
# data.csv
# Duration,Pulse,Maxpulse,Calories
# 60,110,130,409.1
# 60,117,145,479.0
# 60,103,135,340.0
# 45,109,175,282.4
# 45,117,148,406.0

import pandas as pd
df = pd.read_csv("data.csv")

# 1️⃣  Stack the (only) column level ↓ into the index
long = df.stack()          # returns a Series because no columns remain
print(long.to_string())    # show everything without truncation
----
.Output:
[source,python]
----
0  Duration     60.0
   Pulse       110.0
   Maxpulse    130.0
   Calories    409.1
1  Duration     60.0
   Pulse       117.0
   Maxpulse    145.0
   Calories    479.0
2  Duration     60.0
   Pulse       103.0
   Maxpulse    135.0
   Calories    340.0
...
dtype: float64
----

.How it works
* The original columns (`Duration`, `Pulse`, `Maxpulse`, `Calories`) form a **single** column level.
* `stack()` moves that level onto the row axis, creating a *MultiIndex* Series whose index is `(row-number, former-column-label)`.
* Because no column levels are left, pandas returns a `Series`.  
  If at least one column level remains (true MultiIndex columns), the result would be a `DataFrame`.

*When to Use:*  
- Quickly convert a *wide* table to a *long* Series without needing a multi-level column index.  
- Feed “tidy” tools that expect a single measurement per row (e.g., seaborn, `groupby`, `agg`).  

*When Not to Use:*  
- If columns already carry **meaningful levels** (e.g., `("Metric","Month")`) and you only want to pivot one of them—then keep the MultiIndex and specify `stack(level='Month')`.  
- If you merely need to reshape by **row index** → use `unstack()` or `pivot()` instead.

.Tips  
* Call `long.unstack()` to reverse the operation (round-trip check).  
* After stacking, `long.reset_index()` will turn the MultiIndex into regular columns for easier CSV export.  
* Remember: with only one column level, `stack()` always yields a Series; multi-level columns yield a DataFrame.  



==== df.pivot()  – reshape **without** aggregation
.Creates a “spread-out” table; every unique combination of index/column must be unique.
[source,python]
----
import pandas as pd
df = pd.DataFrame({
    "id"      : [1, 1, 2, 2, 3, 3],
    "category": ["A", "B", "A", "B", "A", "B"],
    "value"   : [10, 20, 15, 25, 12, 18]
})
print(df)
# 1️⃣  Put category values into separate columns (wide format)
wide = df.pivot(index="id",        # rows
                columns="category",# new columns come from here
                values="value")    # what goes into the cells
print(wide)
----
.Output:
[source,python]
----
   id category  value
0   1        A     10
1   1        B     20
2   2        A     15
3   2        B     25
4   3        A     12
5   3        B     18
category   A   B
id              
1         10  20
2         15  25
3         12  18
----
*Key points*
* Every (id, category) pair must appear **once** in `df`; duplicates raise `ValueError`.
* `pivot()` performs **no** aggregation – it just rearranges.

*When to use:*  
- Your data are already unique per key pair (e.g. one measurement per day and sensor).  
- You want a quick wide-format DataFrame for plotting or simple math.

*When not to use:*  
- There are duplicates – switch to `pivot_table()` and pick an aggregation.



==== pd.pivot_table()  – pivot **with** aggregation
.Like a spreadsheet pivot table: duplicates are allowed, aggregated with `aggfunc`.
[source,python]
----
# ─── same data but with duplicates ─────────────────────
df_dup = pd.DataFrame({
    "id"      : [1, 1, 1, 2, 2, 3],
    "category": ["A", "A", "B", "A", "B", "B"],
    "value"   : [10, 5, 20, 15, 25, 18]
})

# 1️⃣  Sum up duplicate rows while pivoting
pivot_sum = pd.pivot_table(
    df_dup,
    index   ="id",
    columns ="category",
    values  ="value",
    aggfunc ="sum"          # any NumPy / pandas reduction works
)
print(pivot_sum)
----
.Output:
[source,python]
----
category   A   B
id              
1         15  20
2         15  25
3        NaN  18
----
*Key points*
* Accepts duplicates – they’re collapsed with `aggfunc` (default is **mean**).  
* Missing combinations become `NaN` (id 3 has no ‘A’).

.Variations
• Multiple aggregations:  
  `aggfunc = {"value": ["sum", "count"]}` → MultiIndex columns  
• Add totals like Excel: `margins=True` (adds “All” row/col).  
• Pivot on many columns: `index=["id","week"]`, `columns=["category","subcat"]`.

*When to use:*  
- You need summary numbers (sum, mean, count, etc.) while reshaping.  
- Data contain duplicates or you explicitly want aggregation (e.g., daily sales per store).

*When not to use:*  
- The key pairs are unique and you just need a reshape – plain `pivot()` is faster and simpler.


==== Difference between pivot() and melt()
.Short version:
* melt = “un-pivot” → Wide → Long.
* pivot = “re-pivot” → Long → Wide.

.Key Contrast:
1. Direction:
    - melt turns several columns into two: variable + value
    - pivot takes one variable column and turns its distinct labels into real columns

2. Uniqueness vs duplicates:
    - melt never complains about duplicates; it just stacks rows.
    - pivot raises ValueError if (index, column) is seen twice. Use pivot_table when duplicates exist because it can aggregate.

3. Aggregation:
    - melt does not aggregate; it just reshapes.
    - pivot requires unique pairs, while pivot_table aggregates duplicates.


=== Tidy Data

==== Error Type 1 “Headers are actually data values”
Example:
[source,python]
----
           name  treatmenta  treatmentb
0    John Smith         NaN           2
1      Jane Doe        16.0          11
2  Mary Johnson         3.0           1
----
.Why is it a problem?

* The treatment name (a, b) is a variable, but it sits in the column headers.
* Each patient appears once per treatment, so one real observation is split across two columns.

.Fix:
Un-pivot / melt so the treatment goes into its own column and the numeric result into another:
[source,python]
----
import pandas as pd
import numpy as np

# Create the original DataFrame (headers are actually data values)
df = pd.DataFrame({
    'name': ['John Smith', 'Jane Doe', 'Mary Johnson'],
    'treatmenta': [np.nan, 16, 3],
    'treatmentb': [2, 11, 1]
})
# Unpivot/melt so the treatment goes into its own column and the result into another
tidy = (
    df.melt(id_vars='name', var_name='trt', value_name='result')
)

print("\nTidy DataFrame:")
print(tidy)
----

.Output:
[source,python] 
----
Tidy DataFrame:
           name         trt  result
0    John Smith  treatmenta     NaN
1      Jane Doe  treatmenta    16.0
2  Mary Johnson  treatmenta     3.0
3    John Smith  treatmentb     2.0
4      Jane Doe  treatmentb    11.0
5  Mary Johnson  treatmentb     1.0
----



==== Error Type 2  – “Several variables crammed into one column”
.Example:
[source,python] 
----
machine   date     key   value
M       1/1/22    tmin   15.00
M       1/1/22    tmax   17.00
----

.Why is it a problem?
* Column key stores which variable the number belongs to (tmin vs tmax).
* Each physical record now uses two rows to hold one logical observation (min + max temp).

.Fix:
Pivot those key/value rows back out into separate columns:
[source,python]
----
import pandas as pd

# Create the original DataFrame (key column holds variable names)
df = pd.DataFrame({
    'machine': ['M', 'M'],
    'date': ['1/1/22', '1/1/22'],
    'key': ['tmin', 'tmax'],
    'value': [15.00, 17.00]
})

print("Original DataFrame:")
print(df)

# Pivot the key/value rows back out into separate columns
wide = (
    df.pivot(index=['machine', 'date'], columns='key', values='value')
      .reset_index()  # optional, removes MultiIndex
)

print("\nWide DataFrame (tidy):")
print(wide)
----

.Output:
[source,python]
----
Original DataFrame:
  machine    date   key  value
0       M  1/1/22  tmin   15.0
1       M  1/1/22  tmax   17.0

Wide DataFrame (tidy):
key machine    date  tmax  tmin
0         M  1/1/22  17.0  15.0
----


==== Error Type 4 (Surprise? there is no 3) -  “Each observation spread over many rows that need aggregation”
.Example:
[source,python]
----
Store  Date     TransactionID   Amount
1    1/3/22      kdjvi           787.00
1    1/3/22      kjdfj          1887.40
2    1/3/22      geoku          1148.30
----

.Why is it a problem?
* For analysis you care about the monthly revenue per store, not each receipt line; the granularity is too fine.

.Fix:
Aggregate (group + sum) then drop the now-irrelevant columns:
[source, python]
----
import pandas as pd

# Create the original DataFrame (each transaction is a row)
df = pd.DataFrame({
    'Store': [1, 1, 2],
    'Date': ['1/3/22', '1/3/22', '1/3/22'],
    'TransactionID': ['kdjvi', 'kjdfj', 'geoku'],
    'Amount': [787.00, 1887.40, 1148.30]
})

print("Original DataFrame:")
print(df)

# Aggregate: group by Store and Month, sum Amount
tidy = (
    df.assign(Month=pd.to_datetime(df['Date']).dt.to_period('M'))
      .groupby(['Store', 'Month'], as_index=False)['Amount']
      .sum()
)

print("\nTidy DataFrame (monthly revenue per store):")
print(tidy)
----

.Output:
[source,python]
----
Original DataFrame:
    Store    Date TransactionID  Amount
0      1  1/3/22         kdjvi   787.0
1      1  1/3/22         kjdfj  1887.4
2      2  1/3/22         geoku  1148.3

Tidy DataFrame (monthly revenue per store):
    Store    Month  Amount
0      1  2022-01  2674.4
1      2  2022-01  1148.3
----



==== Summary
Summary cheat-sheet
* Headers-as-data → melt() (longer).
* Many variables in one column → pivot() / pivot_table() (wider).
* Too-fine granularity / repeated rows → groupby(...).agg() to collapse.

